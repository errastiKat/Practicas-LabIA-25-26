{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48txrMHGTK91"
      },
      "source": [
        "## üõë Complete los siguientes datos\n",
        "\n",
        "- **Nombre y apellidos:**  Katrin Mu√±oz Errasti\n",
        "- **Entrega:** Entrega 04 - Procesamiento de texto\n",
        "- **Fecha:**  16/11/2025\n",
        "- **Tiempo dedicado a la entrega:**  \n",
        "\n",
        "- **Principales dudas y dificultades encontradas en el desarrollo:**  \n",
        "  Durante la pr√°ctica tuve sobre todo tres  dificultades:\n",
        "  \n",
        "  1. **Coherencia del preprocesado entre apartados**  \n",
        "     Me cost√≥ asegurarme de que cada modelo usaba la versi√≥n de texto adecuada (original, filtrado b√°sico, etc.). En particular, comet√≠ el error de entrenar Word2Vec sobre el texto preprocesado mientras el `Tokenizer` de Keras se ajustaba sobre el texto original, lo que gener√≥ un desajuste de vocabulario que luego tuve que detectar y corregir en la redacci√≥n y en el c√≥digo.\n",
        "\n",
        "  2. **Limitaciones de GPU y tiempos de entrenamiento**  \n",
        "     En varios experimentos con LSTM y, sobre todo, con los Transformers, Colab se quedaba sin GPU o me dejaba sin cuota. Para poder completar todos los experimentos tuve que recurrir a otras cuentas personales para seguir disponiendo de GPU y terminar de entrenar los modelos m√°s pesados.\n",
        "\n",
        "  3. **Apartado extra de atenciones**  \n",
        "     El requisito extra fue la parte m√°s exigente: no solo por integrar `bertviz` y conseguir que devolviera correctamente los mapas de atenci√≥n, sino tambi√©n por interpretar de forma razonada qu√© estaba ‚Äúmirando‚Äù el modelo en aciertos y errores. Me llev√≥ bastante tiempo seleccionar ejemplos representativos, depurar la funci√≥n de visualizaci√≥n y conectar esas observaciones con las conclusiones finales de la pr√°ctica.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JzBiXMLnKXwu"
      },
      "source": [
        "#Instrucciones Generales\n",
        "\n",
        "**Laboratorio de IA**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eUhMN-zEVOQf"
      },
      "source": [
        "**Contexto de la pr√°ctica, preliminares e instrucciones**\n",
        "\n",
        "Este es el cuaderno que utilizar√°s como **plantilla** para entrega de la asignatura Laboratorio de IA del tema correpondiente.\n",
        "\n",
        "La pr√°ctica est√° **alineada** con las sesiones de teor√≠a y el ejemplo de c√≥digo visto en el aula, por lo que es recomendable un repaso al material del tema previo a la implementaci√≥n de la pr√°ctica.\n",
        "\n",
        "Por favor, **lee atentamente** el enunciado. Si tienes alguna duda, utiliza el foro o ponte en contacto con el profesor (pero no compartas c√≥digo).\n",
        "\n",
        "Consideraciones generales sobre el c√≥digo:\n",
        "- Cuando se le pida que presente un valor o resultado, mu√©strelo con el contexto y la precisi√≥n adecuados, es decir, \"La tabla tiene 100 filas y 4 columnas\" en lugar de mostrar \"100 4\" por pantalla o \"precisi√≥n del 66,7%\" en lugar de simplemente imprimir \"0,66666666\".\n",
        "- Utilice tantas celdas (de texto o de c√≥digo) como considere para dar una respuesta legible y clara a las preguntas planteadas.\n",
        "- Mostrar resultados intermedios o finales (tablas, contenido de las variables...) que demuestren que la soluci√≥n es correcta si fuera necesario.\n",
        "- No olvide importar todas las librer√≠as necesarias para la correcta ejecuci√≥n del c√≥digo, incluyendo la instalaci√≥n de las mismas, si fuera necesario.\n",
        "\n",
        "\n",
        "**Para entregar el cuaderno:**\n",
        "- Impr√≠melo en un archivo PDF con todas las secciones expandidas y todas las celdas ejecutadas, de forma que se pueda ver todo el c√≥digo y se muestren todos los resultados.\n",
        "- Guardar el cuaderno como archivo ipynb.\n",
        "\n",
        "Entregue ambos archivo a ALUD sin comprimir. Nombre los ficheros con su nombre, apellidos y el n√∫mero de pr√°ctica.\n",
        "\n",
        "**Evaluaci√≥n:**  Esta pr√°ctica cuenta un 11.1% de la evaluaci√≥n final de la asignatura.\n",
        "\n",
        "En las indicaciones, hay instrucciones extra indicadas con el s√≠mbolo ü§ñ. No son requisitos para superar la pr√°ctica, sino acciones que puedes realizar para mejorar la calificaci√≥n.\n",
        "\n",
        "La calificaci√≥n de los ejercicios se har√° con los siguientes criterios:\n",
        "- [9 a 10] puntos: La soluci√≥n aportada cumple con los requisitos de forma excelente y existen aspectos positivos o extras (ü§ñ) que avalan su excelencia.\n",
        "- [7 a 9] puntos: La soluci√≥n aportada cumple con los requisitos, pero algunas cuestiones menores son susceptibles de ser mejoradas, tales como: presentaci√≥n de la soluci√≥n, justificaci√≥n de la decisi√≥n tomada, eficiencia del c√≥digo...\n",
        "- [5 a 7] puntos: Las soluciones aportadas no cumplen alguno de los requisitos, como por ejemplo: no se responde a una pregunta, no se aporta la soluci√≥n a una parte...\n",
        "- [1 a 5] puntos: La soluci√≥n aportada no cumple con varios requisitos.\n",
        "- [0] puntos: soluci√≥n no aportada o soluci√≥n plagiada.\n",
        "\n",
        "La limpieza y la eficiencia del c√≥digo, as√≠ como las explicaciones dadas ser√°n tenidas en cuenta  para la evaluaci√≥n.\n",
        "\n",
        "**Plazo:** El plazo de entrega de la pr√°ctica finaliza el 14 de noviembre de 2025.\n",
        "\n",
        "Ten en cuenta que la pr√°ctica podr√° entregarse fuera del plazo establecido, en cualquier momento del cuatrimestre. No obstante, las entregas tard√≠as se evaluar√°n sobre un m√°ximo del 75 % de la calificaci√≥n en la convocatoria ordinaria (si se entregan antes del examen final) o en la convocatoria extraordinaria (si se entregan entre el examen final y el examen extraordinario)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-YPrfQZnS4_6"
      },
      "source": [
        "# üõë Desarrollo a realizar\n",
        "\n",
        "La pr√°ctica tiene como finalidad aplicar de forma integrada los contenidos vistos en clase, desde el an√°lisis exploratorio hasta el uso de modelos avanzados de lenguaje. Se evaluar√° no solo la correcta implementaci√≥n t√©cnica, sino la capacidad de an√°lisis, interpretaci√≥n y justificaci√≥n de los resultados.\n",
        "\n",
        "<font color=\"red\">‚ö†Ô∏è Importante: si no se utilizan las t√©cnicas ense√±adas en clase y no se responden todas las preguntas planteadas con razonamiento propio, la pr√°ctica no ser√° evaluada. El objetivo es comprobar comprensi√≥n, no reproducir contenido generado autom√°ticamente. </font>\n",
        "\n",
        "En las pr√≥ximas celdas tendr√°s que trabajar con el siguiente [dataset](https://drive.google.com/file/d/1qPWUpwgThU5SAhqbWxVgKkvrzLtR9D8r/) en el que aparecen textos generados por IA (1) o por humanos (0)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Utilizando las librer√≠as y t√©cnicas vistas en clase**, completa las siguientes tareas:\n",
        "\n",
        "\n",
        "**Requisitos b√°sicos**\n",
        "\n",
        " 1. Realizar un EDA b√°sico. Por ejemplo:\n",
        "- Realizar un an√°lisis exploratorio del corpus:\n",
        "- Distribuci√≥n de longitudes de los textos.\n",
        "- Palabras m√°s frecuentes, bigramas y trigramas m√°s comunes.\n",
        "- Visualizaciones: histogramas, nubes de palabras, gr√°ficos de frecuencia.\n",
        "\n",
        "Requisito: deben extraerse conclusiones razonadas a partir de los resultados. Ejemplo: qu√© tipo de vocabulario predomina, presencia de polaridad l√©xica, sesgos o patrones de estilo. Si no se presentan conclusiones interpretables, el apartado no ser√° v√°lido.\n",
        "\n",
        "\n",
        " 2. Modelos de Aprendizaje Autom√°tico Cl√°sico\n",
        "\n",
        "- Entrenar un modelo cl√°sico de clasificaci√≥n como Logistic Regression, Naive Bayes y SVM (u otros justificados).\n",
        "- Comparar su rendimiento con tres tipos de representaciones:\n",
        "   - TF-IDF\n",
        "   - Bolsa de Palabras (Bag of Words)\n",
        "   - Word2Vec o embeddings no contextuales (propios o preentrenados).\n",
        "Evaluar con m√©tricas: accuracy, F1-score, matriz de confusi√≥n.\n",
        "\n",
        "Requisito: deben extraerse conclusiones y justificar las diferencias entre modelos y representaciones. Por ejemplo, por qu√© el accuracy mejora o empeora al usar embeddings frente a TF-IDF, qu√© tipo de errores cometen los modelos y a qu√© se deben.\n",
        "\n",
        "\n",
        " 3.  Modelado de T√≥picos (Topic Modelling)\n",
        "\n",
        "- Implementar y comparar LDA y HDP.\n",
        "- Evaluar la coherencia de los t√≥picos mediante m√©tricas o inspecci√≥n cualitativa.\n",
        "- Relacionar los t√≥picos obtenidos con las etiquetas o clases del dataset interpretando posibles correspondencias.\n",
        "- Presentar ejemplos de textos representativos de cada t√≥pico y discutir su coherencia sem√°ntica.\n",
        "\n",
        "4. Implementar una red LSTM:\n",
        "\n",
        "- Usando embeddings de la red y embeddings Word2Vec ajustables.\n",
        "- Comparar el rendimiento con los modelos cl√°sicos\n",
        "- Justificar las diferencias observadas.  \n",
        "\n",
        "5. Realizar dos experimentos de fine-tuning con un modelo Transformer:\n",
        "- Fine-tuning ligero: solo ajustar las capas finales.\n",
        "- Fine-tuning completo: ajustar todo el modelo.\n",
        "- Comparar resultados, tiempos de entrenamiento y capacidad de generalizaci√≥n.\n",
        "- Explicar las diferencias mediante inferencias basadas en el tipo de representaci√≥n contextual, el tama√±o del modelo y la cantidad de datos.\n",
        "    \n",
        "**Consideraciones deseables**\n",
        "\n",
        "6. Elaborar una discusi√≥n integradora que responda:\n",
        "    -  ¬øQu√© t√©cnicas funcionaron mejor y en qu√© condiciones?\n",
        "    -  ¬øQu√© limitaciones se observaron en los modelos cl√°sicos frente a los basados en DL o Transformers?\n",
        "    -  ¬øQu√© trade-offs existen entre interpretabilidad, coste computacional y rendimiento?\n",
        "    -  Presentar visualizaciones complementarias.\n",
        "    -  Incluir una reflexi√≥n cr√≠tica final sobre los resultados y el aprendizaje obtenido.\n",
        "\n",
        "     \n",
        "**ü§ñRequisitos extra**\n",
        "\n",
        "- Realiza un an√°lisis e interpretaci√≥n de las [atenciones internas](https://github.com/jessevig/bertviz) (attention weights) de uno de los modelo Transformer que hayas fine-tuneado para clasificaci√≥n.\n",
        "- El objetivo es visualizar y comentar qu√© partes del texto reciben mayor atenci√≥n en distintos ejemplos (aciertos y errores), y extraer conclusiones sobre c√≥mo el modelo toma sus decisiones.\n",
        "\n",
        "Requisito: No se aceptar√° el apartado si solo se muestran gr√°ficos sin interpretaci√≥n."
      ],
      "metadata": {
        "id": "aA7Qj3L8J0Zy"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMKUU-a3lk00"
      },
      "source": [
        "# **0. Preparaci√≥n del entorno**\n",
        "Esta secci√≥n sirve para instalar los paquetes e importar las librer√≠as que se van a usar durante la pr√°ctica."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k-vNKrK3-qdi"
      },
      "source": [
        "## 0.1. Instalar paquetes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0UKAfCmpA1eS"
      },
      "outputs": [],
      "source": [
        "!pip install nltk scikit-learn gensim wordcloud tensorflow transformers datasets bertviz evaluate -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E8o3DUmvA7TJ"
      },
      "source": [
        "## 0.2. Importar Librer√≠as"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VGg9ObtnBA14"
      },
      "outputs": [],
      "source": [
        "# Imports generales\n",
        "import os\n",
        "import re\n",
        "import time\n",
        "import warnings\n",
        "from collections import Counter\n",
        "from typing import List, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Requisito 1: EDA b√°sico\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.util import ngrams\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "# Configuraci√≥n NLTK\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('stopwords', quiet=True)\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt_tab')\n",
        "except LookupError:\n",
        "    nltk.download('punkt_tab', quiet=True)\n",
        "\n",
        "# Requisito 2: Modelos cl√°sicos de ML\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    classification_report,\n",
        "    confusion_matrix,\n",
        "    f1_score,\n",
        "    precision_recall_fscore_support,\n",
        ")\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "# Requisito 3: Modelado de T√≥picos\n",
        "from gensim.corpora import Dictionary\n",
        "from gensim.models import LdaModel, HdpModel\n",
        "from gensim.models.coherencemodel import CoherenceModel\n",
        "\n",
        "# Requisito 4: Red LSTM\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import (\n",
        "    Bidirectional,\n",
        "    Dense,\n",
        "    Dropout,\n",
        "    Embedding,\n",
        "    GlobalMaxPooling1D,\n",
        "    LSTM,\n",
        "    SpatialDropout1D,\n",
        ")\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# Requisito 5: Fine-tuning de Transformers\n",
        "import torch\n",
        "from datasets import ClassLabel, Dataset, Features, Value\n",
        "from evaluate import load as load_metric\n",
        "from transformers import (\n",
        "    AutoModelForSequenceClassification,\n",
        "    AutoTokenizer,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        ")\n",
        "from IPython.display import display\n",
        "\n",
        "# Requisito extra: An√°lisis de atenciones\n",
        "from bertviz import head_view\n",
        "\n",
        "# Configuraci√≥n de plots y constantes de visualizaci√≥n\n",
        "sns.set_style('whitegrid')\n",
        "plt.rcParams['figure.figsize'] = (12, 6)\n",
        "\n",
        "# Verde para humano y azul para IA en las visualizaciones\n",
        "ColorHumano = \"#2CA02C\"\n",
        "ColorIA     = \"#1F77B4\"\n",
        "\n",
        "# Etiquetas para matrices de confusi√≥n\n",
        "LabelNames = [\"Humano\", \"IA\"]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2YZ2zk2xBGz5"
      },
      "source": [
        "## 0.3. Cargar y preprocesar Dataset\n",
        "\n",
        "En este apartado, se lee el fichero `Lab_IA_NLP_task.csv` que contiene los textos y su etiqueta (0 = humano, 1 = IA).\n",
        "\n",
        "Antes de avanzar con el an√°lisis es necesario hacer algunas comprobaciones para evitar errores en el resto de apartados:\n",
        "  * Comprobar si el dataset se ha cargado correctamente\n",
        "  * Comprobar si las columnas tienen el tipo de variable adecuada\n",
        "  * Comprobar que no hay valores nulos\n",
        "  * Comprobar si los grupos est√°n equilibrados\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d0-jiBX8BMxf"
      },
      "outputs": [],
      "source": [
        "# Cargar el Dataset\n",
        "df = pd.read_csv(\"Lab_IA_NLP_task.csv\", encoding='utf-8')\n",
        "\n",
        "# Comprobar que se ha cargado correctamente\n",
        "print(f\"Dataset cargado, total de filas: {len(df)}\")\n",
        "print(\"\\nPrimeras 5 filas:\")\n",
        "print(df.head())\n",
        "\n",
        "# Comprobar tipos de datos\n",
        "print(\"\\nInformaci√≥n del DataFrame:\")\n",
        "df.info()\n",
        "\n",
        "# Comprobar si hay valores nulos\n",
        "print(f\"\\nNulos en 'contenido': {df['contenido'].isna().sum()}\")\n",
        "print(f\"Nulos en 'etiqueta': {df['etiqueta'].isna().sum()}\")\n",
        "\n",
        "# Eliminar filas donde el texto es nulo\n",
        "df.dropna(subset=['contenido'], inplace=True)\n",
        "\n",
        "# Comprobar la distribuci√≥n de clases\n",
        "print(\"\\nDistribuci√≥n de clases (0=Humano, 1=IA):\")\n",
        "print(df['etiqueta'].value_counts(normalize=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2MUaBKP-BUrn"
      },
      "source": [
        "\n",
        "# **1. Realizar un EDA b√°sico**\n",
        "\n",
        "Este apartado tiene como objetivo realizar un An√°lisis Exploratorio de Datos (EDA). Es decir, entender nuestro corpus antes de aplicar un modelo.\n",
        "\n",
        "El EDA permite encontrar patrones y caracter√≠sticas en los datos. En este contexto, la meta es encontrar diferencias visibles entre los textos escritos por humanos (Clase 0) y los generados por IA (Clase 1).\n",
        "\n",
        "Para ello, se analizan:\n",
        "\n",
        "  * **Longitud de los textos** originales y tras el preprocesado\n",
        "  * **Efecto del preprocesamiento** sobre el corpus\n",
        "  * **Frecuencia de N-gramas** para cada clase\n",
        "  * **Nubes de palabras**\n",
        "  * **Gr√°ficos de frecuencia** de los t√©rminos m√°s representativos de cada clase\n",
        "  * **Riqueza de vocabulario** mediante la m√©trica *Type Token Ratio* (TTR).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qcZMZM9gqbHR"
      },
      "source": [
        "## 1.1. Preprocesamiento b√°sico para el EDA\n",
        "\n",
        "Antes de analizar el texto es necesario realizar un preprocesado. En este caso se siguen tres pasos principales:\n",
        "\n",
        "1. **Tokenizaci√≥n**: separar las frases (cadenas de texto) en una lista de tokens (palabras).\n",
        "2. **Limpieza**: convertir todo a min√∫sculas y eliminar signos de puntuaci√≥n y n√∫meros.\n",
        "3. **Filtrado de *stopwords***: una *stopword* es una palabra muy frecuente que no aporta un significado distintivo al texto (art√≠culos, preposiciones, conjunciones, etc.).\n",
        "\n",
        "En la primera versi√≥n del preprocesado se aplic√≥ √∫nicamente el conjunto de *stopwords* en espa√±ol que proporciona NLTK. Este planteamiento es un buen punto de partida porque elimina las palabras funcionales m√°s frecuentes del idioma y permite realizar un an√°lisis exploratorio inicial.\n",
        "\n",
        "No obstante, al obtener los n-gramas y revisar las frecuencias se observ√≥ que, en este corpus concreto, aparec√≠an con mucha frecuencia t√©rminos que, aun no siendo *stopwords* ‚Äúoficiales‚Äù, se comportan como tales en este conjunto de datos (por ejemplo, ‚Äúpuede‚Äù, ‚Äúimportante‚Äù, ‚Äúprimero‚Äù). Este tipo de muletillas procede del car√°cter instructivo de muchos de los textos y tiende a ocultar otros patrones m√°s espec√≠ficos.\n",
        "\n",
        "Por este motivo se decidi√≥ aplicar un **doble filtrado**:\n",
        "\n",
        "  * **Filtro b√°sico**: aplica √∫nicamente las *stopwords* de NLTK. Se utilizar√° en los apartados donde interesa conservar la mayor cantidad posible de informaci√≥n l√©xica (por ejemplo, para las representaciones que se emplear√°n en los modelos)\n",
        "  * **Filtro extendido**: parte del filtrado b√°sico y a√±ade un peque√±o conjunto de palabras muy frecuentes detectadas en este dataset (las muletillas). Se emplear√° en las secciones de an√°lisis cualitativo (N-gramas y nubes de palabras) con el fin de obtener listados m√°s informativos y menos dominados por estructuras repetitivas\n",
        "\n",
        "De este modo se mantiene un preprocesado est√°ndar para los modelos y, al mismo tiempo, se adapta la limpieza al contenido concreto del corpus cuando el objetivo es interpretar los resultados.\n",
        "\n",
        "A partir del recuento de longitudes se puede observar que el filtrado b√°sico reduce la longitud media de los textos en torno a la mitad y que el filtrado extendido realiza una reducci√≥n adicional m√°s peque√±a, lo que confirma que las muletillas detectadas eran frecuentes pero no dominantes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JRig0PzeBeRU"
      },
      "outputs": [],
      "source": [
        "# Stopwords b√°sicas de NLTK (espa√±ol)\n",
        "StopWordsES = set(stopwords.words('spanish'))\n",
        "\n",
        "# Conjunto de palabras muy frecuentes del dataset\n",
        "ExtraStopWords = {\n",
        "    # muletillas generales\n",
        "    \"puede\", \"pueden\", \"puedes\",\n",
        "    \"ser\", \"si\",\n",
        "    \"importante\",\n",
        "    \"c√≥mo\",\n",
        "    \"tener\", \"tienes\",\n",
        "    # estructura de tutorial\n",
        "    \"primero\", \"primer\",\n",
        "    \"debes\", \"debe\",\n",
        "    \"continuaci√≥n\", \"presentamos\",\n",
        "    \"aqu√≠\",\n",
        "    \"manera\",\n",
        "    \"forma\",\n",
        "    \"lugar\",\n",
        "    \"art√≠culo\",\n",
        "    \"consejos\",\n",
        "    # ruido del dataset\n",
        "    \"x\",\n",
        "    \"ciervos\",\n",
        "    \"pichafuera\"\n",
        "}\n",
        "\n",
        "# Stopwords extendidas\n",
        "StopWordsExtendido = StopWordsES.union(ExtraStopWords)\n",
        "\n",
        "# Preprocesado b√°sico\n",
        "df[\"ContenidoBasico\"] = (\n",
        "    df[\"contenido\"]\n",
        "    # Pasar todo a min√∫sculas\n",
        "    .str.lower()\n",
        "    # Eliminar signos de puntuaci√≥n + caracteres no alfanum√©ricos\n",
        "    .str.replace(r\"[^\\w\\s]\", \"\", regex=True)\n",
        "    # Tokenizar\n",
        "    .apply(word_tokenize)\n",
        "    # Texto filtrado con stopwords b√°sicos\n",
        "    .apply(lambda toks: \" \".join([t for t in toks if t.isalpha() and t not in StopWordsES]))\n",
        ")\n",
        "\n",
        "# Preprocesado extendido\n",
        "df[\"ContenidoFiltrado\"] = (\n",
        "    df[\"contenido\"]\n",
        "    .str.lower()\n",
        "    .str.replace(r\"[^\\w\\s]\", \"\", regex=True)\n",
        "    .apply(word_tokenize)\n",
        "    # Texto filtrado con stopwords extendidos\n",
        "    .apply(lambda toks: \" \".join([t for t in toks if t.isalpha() and t not in StopWordsExtendido]))\n",
        ")\n",
        "\n",
        "# Comparar longitudes\n",
        "# N√∫mero  palabras texto original / texto filtrado b√°sico / texto filtrado extendido\n",
        "df[\"LongitudOriginal\"] = df[\"contenido\"].str.split().str.len()\n",
        "df[\"LongitudBasico\"] = df[\"ContenidoBasico\"].str.split().str.len()\n",
        "df[\"LongitudExtendido\"] = df[\"ContenidoFiltrado\"].str.split().str.len()\n",
        "\n",
        "print(df[[\"LongitudOriginal\", \"LongitudBasico\", \"LongitudExtendido\"]].describe())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dKf1y437qwZk"
      },
      "source": [
        "###1.1.1 An√°lisis del efecto del preprocesamiento\n",
        "El preprocesamiento se ha aplicado sobre el total de los textos (9037.000). Se han generado 3 versiones para cada texto: el original, con filtrado b√°sico y con filtrado extendido. Los resultados obtenidos se pueden ver en la tabla de arriba.\n",
        "\n",
        "La reducci√≥n que se puede ver al pasar de la versi√≥n original a la versi√≥n con filtrado b√°sico es cercana al 50%, lo que muestra que el corpus incial conten√≠a una gran cantidad de elementos funcionales. En cambio, el filtrado extendido, solo reduce unas 3 palabras adicionales de media, lo que significa que las muletillas espec√≠ficas que se eliminaron en el segundo filtrado est√°n presentes en el texto pero no son dominantes.\n",
        "\n",
        "Es interesante fijarse en la m√≠nima longitud ya que se pueden observar textos cuya longitud es 0 tras el filtrado por esta raz√≥n en los entrenamientos se descartan las frases vacias."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mw3LpNxIBj4f"
      },
      "source": [
        "## 1.2. Distribuci√≥n de Longitudes de los Textos\n",
        "\n",
        "El primer an√°lisis consiste en comprobar si existen diferencias de longitud entre los textos humanos y los generados por IA.\n",
        "\n",
        "La longitud es una caracter√≠stica num√©rica sencilla que un modelo de Machine Learning (ML) puede aprovechar, y adem√°s nos permite detectar si una de las clases tiende a producir textos m√°s largos o m√°s cortos.\n",
        "\n",
        "Se utiliza la longitud del texto original porque se trata de una propiedad intr√≠nseca al dato tal y como viene en el dataset, antes de cualquier limpieza. De este modo se compara de forma realista el tama√±o de los textos de cada clase. Para visualizar la diferencia se representa un histograma separado por clases."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gICvssa3Bn21"
      },
      "outputs": [],
      "source": [
        "df['LongitudTexto'] = df['contenido'].apply(len)\n",
        "\n",
        "plt.figure()\n",
        "sns.histplot(\n",
        "    data=df,\n",
        "    x='LongitudTexto',\n",
        "    hue='etiqueta',\n",
        "    palette=[ColorHumano, ColorIA])\n",
        "\n",
        "plt.title('Distribuci√≥n de Longitudes de Texto (Humano, IA)')\n",
        "plt.xlabel('Longitud del Texto (caracteres)')\n",
        "plt.ylabel('Frecuencia')\n",
        "plt.legend(title='Clases', labels=LabelNames)\n",
        "plt.show()\n",
        "\n",
        "# Resumen estad√≠stico\n",
        "print(\"\\nEstad√≠sticas de longitud por clase:\")\n",
        "print(df.groupby('etiqueta')['LongitudTexto'].describe())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mi2EH7QvsrV4"
      },
      "source": [
        "La longitud de los caracteres se ha analizado por separado tanto para la clase humano como para la clase IA.\n",
        "\n",
        "Las medidas de tendencia central y de dispersi√≥n son muy similares aunque la clase humana tiene m√°s dispersi√≥n (std m√°s alta) debido a unos textos extensos, son una minor√≠a. Esta homogeneidad concluye que la longitud no es un rasgo discriminativo suficiente para separar texto humano de texto generado por IA en este Dataset.\n",
        "\n",
        "Este resultado es coherente con el corpus, ya que todos los textos vienen del mismo tipo de tarea y siguen un formato parecido, explicar un procedimiento.\n",
        "\n",
        "En resumen, la longitud puede usarse como caracter√≠stica auxiliar, pero no tiene capacidad clasificatoria por s√≠ sola."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KzYD3Ie-Buqz"
      },
      "source": [
        "## 1.3. Frecuencia de Palabras\n",
        "\n",
        "\n",
        "El an√°lisis de frecuencia es √∫til para conocer tanto el tema como el estilo del corpus. Para ello no solo se cuentan palabras sueltas, sino tambi√©n N-gramas (secuencias de varias palabras):\n",
        "\n",
        "  * **Unigramas** (1 palabra)\n",
        "    * Vocabulario m√°s habitual\n",
        "    * Temas dominantes\n",
        "  * **Bigramas** (2 palabras)\n",
        "    * Expresiones frecuentes\n",
        "    * Estructuras sencillas\n",
        "  * **Trigramas** (3 palabras)\n",
        "    * Patrones de redacci√≥n\n",
        "    * Formulas repetidas\n",
        "\n",
        "Dado que en el apartado de preprocesado se detect√≥ la presencia de muletillas muy frecuentes en este dataset, el an√°lisis se realiza con los dos filtrados.\n",
        "\n",
        "Con el **filtro b√°sico** para observar el ruido real del corpus y comprobar qu√© expresiones lo dominan.\n",
        "\n",
        "Con el **filtro extendido** para revelar patrones de contenido m√°s espec√≠ficos de cada clase una vez eliminadas esas repeticiones.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AEdWj_I9Bxu3"
      },
      "outputs": [],
      "source": [
        "# Funciones auxiliares para N-gramas\n",
        "def ContarNgramasFrecuentes(corpus: str, n: int, topX: int = 20) -> List[Tuple[str, int]]:\n",
        "    \"\"\" Obtiene los \"topX\" N-gramas m√°s frecuentes de un corpus.\n",
        "        Par√°metros:\n",
        "            corpus: texto completo en formato cadena\n",
        "            n: tama√±o del N-grama\n",
        "            topX: n√∫mero de N-gramas\n",
        "        Devuelve:\n",
        "            Lista de tuplas (n_grama, frecuencia) ordenada de mayor a menor frecuencia.\n",
        "    \"\"\"\n",
        "    # Dividir el texto en tokens\n",
        "    tokens = word_tokenize(corpus)\n",
        "\n",
        "    # Generar lista de N-gramas (si es 1 los N-gramas son los propios tokens)\n",
        "    if n == 1:\n",
        "        n_grams = tokens\n",
        "    else:\n",
        "        n_grams = list(ngrams(tokens, n))\n",
        "\n",
        "    # Contar cuantas veces aparece cada N-grama\n",
        "    freq = Counter(n_grams)\n",
        "    return freq.most_common(topX)\n",
        "\n",
        "\n",
        "def CrearTablaNgramas(ngram_list, nombre_columna=\"N-grama\"):\n",
        "    \"\"\"Convierte una lista de N-gramas a un DataFrame para mostrar los resultados en tablas bien etiquetadas.\"\"\"\n",
        "\n",
        "    # Formatear datos para cuando el N-grama es una tupla\n",
        "    datos = [\n",
        "        (\" \".join(ng) if isinstance(ng, tuple) else ng, freq)\n",
        "        for ng, freq in ngram_list\n",
        "    ]\n",
        "\n",
        "    # Crear DataFrame\n",
        "    df_ng = pd.DataFrame(datos, columns=[nombre_columna, \"Frecuencia\"])\n",
        "    return df_ng\n",
        "\n",
        "\n",
        "def StyleTabla(df, titulo, color_header, color_rows):\n",
        "    \"\"\"Aplica  estilo com√∫n a Dataset\"\"\"\n",
        "    return (\n",
        "        df.style\n",
        "        .hide(axis=\"index\")\n",
        "        .set_caption(titulo)\n",
        "        .set_table_styles([\n",
        "            {\n",
        "                \"selector\": \"caption\",\n",
        "                \"props\": [\n",
        "                    (\"font-weight\", \"bold\"),\n",
        "                    (\"font-size\", \"14px\"),\n",
        "                    (\"margin-bottom\", \"6px\"),\n",
        "                ],\n",
        "            },\n",
        "            {\n",
        "                \"selector\": \"th\",\n",
        "                \"props\": [\n",
        "                    (\"background-color\", color_header),\n",
        "                    (\"color\", \"white\"),\n",
        "                ],\n",
        "            },\n",
        "        ])\n",
        "        .apply(\n",
        "            lambda s: [color_rows if i % 2 == 0 else \"\" for i in range(len(s))],\n",
        "            axis=0,\n",
        "        )\n",
        "    )\n",
        "\n",
        "\n",
        "def MostrarNgramas(corpus_str, n, titulo, color_header, color_row, nombre_columna):\n",
        "    \"\"\"Calcula los top N-gramas de un corpus y los muestra ya formateados en una tabla.\"\"\"\n",
        "    top_list = ContarNgramasFrecuentes(corpus_str, n, 20)\n",
        "    df_ng = CrearTablaNgramas(top_list, nombre_columna=nombre_columna)\n",
        "    display(StyleTabla(df_ng, titulo, color_header, color_row))\n",
        "    return df_ng"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Fa0EF-o4-E0"
      },
      "source": [
        "###1.3.1. An√°lisis de N-gramas (Filtro B√°sico)\n",
        "\n",
        "En este apartado se utilizan los textos preprocesados con el filtro b√°sico para ver el ruido real del corpus y comprobar que muletillas y estructuras dominan las listas de N-gramas en cada clase."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JXch9Q9v2ya_"
      },
      "outputs": [],
      "source": [
        "# Combinar todo el texto preprocesado\n",
        "TextoHumanoBasico = \" \".join(df[df[\"etiqueta\"] == 0][\"ContenidoBasico\"])\n",
        "TextoIABasico    = \" \".join(df[df[\"etiqueta\"] == 1][\"ContenidoBasico\"])\n",
        "\n",
        "# UNIGRAMAS\n",
        "MostrarNgramas(\n",
        "    corpus_str=TextoHumanoBasico,\n",
        "    n=1,\n",
        "    titulo=\"Top unigrama ¬∑ Humano (Filtro b√°sico)\",\n",
        "    color_header=ColorHumano,\n",
        "    color_row=\"background-color: #f3fbf3;\",\n",
        "    nombre_columna=\"Palabra\"\n",
        ")\n",
        "MostrarNgramas(\n",
        "    corpus_str=TextoIABasico,\n",
        "    n=1,\n",
        "    titulo=\"Top unigrama ¬∑ IA (Filtro b√°sico)\",\n",
        "    color_header=ColorIA,\n",
        "    color_row=\"background-color: #f1f6fb;\",\n",
        "    nombre_columna=\"Palabra\"\n",
        ")\n",
        "\n",
        "\n",
        "# BIGRAMAS\n",
        "MostrarNgramas(\n",
        "    corpus_str=TextoHumanoBasico,\n",
        "    n=2,\n",
        "    titulo=\"Top bigramas ¬∑ Humano (Filtro b√°sico)\",\n",
        "    color_header=ColorHumano,\n",
        "    color_row=\"background-color: #f3fbf3;\",\n",
        "    nombre_columna=\"Bigrama\"\n",
        ")\n",
        "MostrarNgramas(\n",
        "    corpus_str=TextoIABasico,\n",
        "    n=2,\n",
        "    titulo=\"Top bigramas ¬∑ IA (Filtro b√°sico)\",\n",
        "    color_header=ColorIA,\n",
        "    color_row=\"background-color: #f1f6fb;\",\n",
        "    nombre_columna=\"Bigrama\"\n",
        ")\n",
        "\n",
        "\n",
        "# TRIGRAMAS\n",
        "MostrarNgramas(\n",
        "    corpus_str=TextoHumanoBasico,\n",
        "    n=3,\n",
        "    titulo=\"Top trigramas ¬∑ Humano (Filtro b√°sico)\",\n",
        "    color_header=ColorHumano,\n",
        "    color_row=\"background-color: #f3fbf3;\",\n",
        "    nombre_columna=\"Trigrama\"\n",
        ")\n",
        "MostrarNgramas(\n",
        "    corpus_str=TextoIABasico,\n",
        "    n=3,\n",
        "    titulo=\"Top trigramas ¬∑ IA (Filtro b√°sico)\",\n",
        "    color_header=ColorIA,\n",
        "    color_row=\"background-color: #f1f6fb;\",\n",
        "    nombre_columna=\"Trigrama\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bLMIc464vDc1"
      },
      "source": [
        "El recuento de unigramas con filtrado b√°sico muestra que ambas clases utilizan frecuentemente verbos modales y de instrucci√≥n. En ambas clases los t√©rminos predominantes coinciden, modales. A√∫n as√≠, en los textos humanos tienen m√°s fuerza palabras de contenido (\"cabello\", \"agua\") mientras que en la clase IA son m√°s frecuentes expresiones t√≠picas de una redacci√≥n guida (\"importante\", \"primero\").\n",
        "\n",
        "Estas diferencias se vuelven m√°s intensas en los bigramas y trigramas. En la clase 1 destacan secuencia claramente procedimentales como, \"debes hacer\" y \"primero debes\" que son formulas muy utilizadas en textos generados. En los textos humanos, aunque se ven expresiones indicativas, tambien aparecen combinaciones relacionadas con objetos reales: \"agua tibia\" o \"peine dientes anchos\".\n",
        "\n",
        "Esto demuestra que aunque el conjunto general tiene una naturaleza instructiva, la clase IA tiende a una redacci√≥n m√°s estandarizada mientras que la otra introduce m√°s referencias espec√≠ficas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RS6ZWaw-5CHh"
      },
      "source": [
        "###1.3.2. An√°lisis de N-gramas (Filtro Extendido)\n",
        "   Se repite el mismo procedimiento pero sobre los textos preprocesados con el filtro extendido. De este modo se obtienen listados m√°s limpios, donde se ven mejor los patrones de contenido y de estilo propios de cada clase.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L37Soba32zEq"
      },
      "outputs": [],
      "source": [
        "# Combinar todo el texto preprocesado (Filtro Extendido)\n",
        "TextoHumanoExtendido = \" \".join(df[df[\"etiqueta\"] == 0][\"ContenidoFiltrado\"])\n",
        "TextoIAExtendido     = \" \".join(df[df[\"etiqueta\"] == 1][\"ContenidoFiltrado\"])\n",
        "\n",
        "# UNIGRAMAS\n",
        "df_uni_h_f = MostrarNgramas(\n",
        "    corpus_str=TextoHumanoExtendido,\n",
        "    n=1,\n",
        "    titulo=\"Top unigrama ¬∑ Humano (Filtro extendido)\",\n",
        "    color_header=ColorHumano,\n",
        "    color_row=\"background-color: #f3fbf3;\",\n",
        "    nombre_columna=\"Palabra\"\n",
        ")\n",
        "df_uni_i_f = MostrarNgramas(\n",
        "    corpus_str=TextoIAExtendido,\n",
        "    n=1,\n",
        "    titulo=\"Top unigrama ¬∑ IA (Filtro extendido)\",\n",
        "    color_header=ColorIA,\n",
        "    color_row=\"background-color: #f1f6fb;\",\n",
        "    nombre_columna=\"Palabra\"\n",
        ")\n",
        "\n",
        "\n",
        "# BIGRAMAS\n",
        "MostrarNgramas(\n",
        "    corpus_str=TextoHumanoExtendido,\n",
        "    n=2,\n",
        "    titulo=\"Top bigramas ¬∑ Humano (Filtro extendido)\",\n",
        "    color_header=ColorHumano,\n",
        "    color_row=\"background-color: #f3fbf3;\",\n",
        "    nombre_columna=\"Bigrama\"\n",
        ")\n",
        "\n",
        "MostrarNgramas(\n",
        "    corpus_str=TextoIAExtendido,\n",
        "    n=2,\n",
        "    titulo=\"Top bigramas ¬∑ IA (Filtro extendido)\",\n",
        "    color_header=ColorIA,\n",
        "    color_row=\"background-color: #f1f6fb;\",\n",
        "    nombre_columna=\"Bigrama\"\n",
        ")\n",
        "\n",
        "\n",
        "# TRIGRAMAS\n",
        "MostrarNgramas(\n",
        "    corpus_str=TextoHumanoExtendido,\n",
        "    n=3,\n",
        "    titulo=\"Top trigramas ¬∑ Humano (Filtro extendido)\",\n",
        "    color_header=ColorHumano,\n",
        "    color_row=\"background-color: #f3fbf3;\",\n",
        "    nombre_columna=\"Trigrama\"\n",
        ")\n",
        "MostrarNgramas(\n",
        "    corpus_str=TextoIAExtendido,\n",
        "    n=3,\n",
        "    titulo=\"Top trigramas ¬∑ IA (Filtro extendido)\",\n",
        "    color_header=ColorIA,\n",
        "    color_row=\"background-color: #f1f6fb;\",\n",
        "    nombre_columna=\"Trigrama\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6MjHAQssxz1e"
      },
      "source": [
        "Una vez eliminadas muletillas del propio corpus el contraste es m√°s n√≠tido y se ve mucho mejor el contenido real.\n",
        "\n",
        "En los textos humanos las primeras posiciones estan ocupadas por verbos de acci√≥n y objeto, tales como, \"cabello\", \"haz clic\", \"usar\" o \"removedor esmalte u√±as\t\". En general, los textos son m√°s concretos y pr√°cticos dan pasos accionables.\n",
        "\n",
        "En los textos de IA el t√©rmino que m√°s se repite es ‚Äúhacer‚Äù (1.081 apariciones), y justo detr√°s aparece con mucha fuerza ‚Äúembargo‚Äù (622 apariciones), junto con otras palabras propias de organizaci√≥n del discurso. En conjunto, esto indica que la IA escribe de forma m√°s discursiva: aunque trate temas similares a los de los textos humanos, tiende a introducir conectores y a seguir una estructura de explicaci√≥n m√°s marcada, con f√≥rmulas recurrentes como ‚Äúparecer tarea desalentadora‚Äù o ‚Äúexplicaremos paso a paso‚Äù.\n",
        "\n",
        "Esto confirma la importancia del doble preprocesado: el primer filtro permite ver el ruido repetitivo del corpus mientras que el segundo deja claras las diferencias de estilo que separan las clases."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xvFJ68QZB1hF"
      },
      "source": [
        "##1.4. Nubes de Palabras\n",
        "\n",
        "Las nubes de palabras permiten representar de forma r√°pida y muy visual que t√©rminos aparecen con mayor frecuencia en un corpus: cuanto m√°s grande aparece una palabra, m√°s veces se encuentra en el corpus. Es importante recalcar que no son un sustituto de las tablas de N-gramas pero son √∫tiles para visualizar el tema y el estilo de los textos.\n",
        "\n",
        "Como en el procesado de filtrado se han generado 2 versiones, se visualizan ambas para comprobar c√≥mo cambia el contenido cuando se eliminan las muletillas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fo3L31QuARFS"
      },
      "source": [
        "###1.4.1. Nubes de Palabras (Filtro B√°sico)\n",
        "En esta versi√≥n se utilizan los textos preprocesados con el filtro b√°sico."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KZpRJTaRB39H"
      },
      "outputs": [],
      "source": [
        "# Nube de Palabras para textos Humanos\n",
        "if TextoHumanoBasico:\n",
        "    WorCloudHumano = WordCloud(width=800, height=400, background_color='white', colormap='Greens').generate(TextoHumanoBasico)\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.imshow(WorCloudHumano, interpolation='bilinear')\n",
        "    plt.axis('off')\n",
        "    plt.title('Nube de Palabras: Textos Humanos (Filtro B√°sico)')\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"No hay texto humano para generar nube de palabras.\")\n",
        "\n",
        "# Nube de Palabras para textos de IA\n",
        "if TextoIABasico:\n",
        "    WorCloudIA = WordCloud(width=800, height=400, background_color='white', colormap='Blues').generate(TextoIABasico)\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.imshow(WorCloudIA, interpolation='bilinear')\n",
        "    plt.axis('off')\n",
        "    plt.title('Nube de Palabras: Textos IA (Filtro B√°sico)')\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"No hay texto de IA para generar nube de palabras.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nOnIsEJR0H-b"
      },
      "source": [
        "Las nubes generadas con el texto con filtrado b√°sico  muestran que las dos clases est√°n dominadas por las mismas palabras funcionales: \"puede\", \"ser\", \"forma\", \"si\".\n",
        "\n",
        "Estas nubes de palabras no muestran gran diferencia visual porque el estio instructivo tiene m√°s peso."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_sN2OOz_Ack-"
      },
      "source": [
        "###1.4.2. Nubes de Palabras (Filtro Extendido)\n",
        "En esta versi√≥n se utilizan los textos preprocesados con el filtro extendido."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rdpAsSGZAfdc"
      },
      "outputs": [],
      "source": [
        "# Nube de Palabras para textos Humanos\n",
        "if TextoHumanoExtendido:\n",
        "    WorCloudHumanoExt = WordCloud(width=800, height=400, background_color='white', colormap='Greens').generate(TextoHumanoExtendido)\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.imshow(WorCloudHumanoExt, interpolation='bilinear')\n",
        "    plt.axis('off')\n",
        "    plt.title('Nube de Palabras: Textos Humanos (Filtro Extendido)')\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"No hay texto humano filtrado para generar nube de palabras.\")\n",
        "\n",
        "# Nube de Palabras para textos IA\n",
        "if TextoIAExtendido:\n",
        "    WorCloudIAExt = WordCloud(width=800, height=400, background_color='white', colormap='Blues').generate(TextoIAExtendido)\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.imshow(WorCloudIAExt, interpolation='bilinear')\n",
        "    plt.axis('off')\n",
        "    plt.title('Nube de Palabras: Textos IA (Filtro Extendido)')\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"No hay texto de IA filtrado para generar nube de palabras.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eRweelPH0t7q"
      },
      "source": [
        "Sin embargo, al usar el texto con filtrado extendido las nubes divergen: en la clase humana predominan palabras orientadas a la ejecuci√≥n de una tarea concreta o objetos (\"cabello\", \"hacer\", \"persona\", \"usar\"), mientras que en la de IA se observan numerosos conectores de explicaci√≥n y t√©rminos utilizados en exposiciones elaboradas (\"hacer\", \"embargo\", \"proceso\")."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yw6UQUk_EF9q"
      },
      "source": [
        "##1.5. Gr√°ficos de Frecuencia\n",
        "\n",
        "Adem√°s de las nubes de palabras, es √∫til ver las palabras m√°s repetidas de forma ordenada. Los gr√°ficos de barras comparan que t√©rminos aparecen m√°s en cada clase y en qu√© volumen.\n",
        "\n",
        "Se calcularon la frecuencia de las palabras de los 2 textos preprocesados. En el caso del filtrado b√°sico, las primeras posiciones las ocupaban palabras muy recurrentes de este corpus (\"puede\", \"importante\", \"primero\") que proceden del caracter instructivo de los textos pero que no aportan informaci√≥n sobre el tema de los textos.\n",
        "\n",
        "Por este motivo, para las visualizaciones decid√≠ mostrar solo el resultado del filtro extendido ya que al eliminar las muletillas se observa claramente el vocabulario representativo de los textos. De este modo, la comparaci√≥n entre clases se centra en t√©rminos de contenido y no en f√≥rmulas recurrentes del corpus, lo que es mucho m√°s interpretable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8a_uOqmZEK7s"
      },
      "outputs": [],
      "source": [
        "# Figura con dos gr√°ficos en paralelo\n",
        "plt.figure(figsize=(20, 10))\n",
        "\n",
        "# Frecuencias para clase Humano\n",
        "ax1 = plt.subplot(1, 2, 1)\n",
        "sns.barplot(\n",
        "    data=df_uni_h_f,\n",
        "    y=\"Palabra\",\n",
        "    x=\"Frecuencia\",\n",
        "    color=ColorHumano\n",
        ")\n",
        "ax1.set_title(\"Top 20 unigramas ¬∑ Humano (filtro extendido)\", fontsize=16, fontweight=\"bold\")\n",
        "ax1.set_xlabel(\"Frecuencia\", fontsize=12)\n",
        "ax1.set_ylabel(\"Palabra\", fontsize=12)\n",
        "ax1.tick_params(labelsize=12)\n",
        "\n",
        "# Frecuencias para clase IA\n",
        "ax2 = plt.subplot(1, 2, 2)\n",
        "sns.barplot(\n",
        "    data=df_uni_i_f,\n",
        "    y=\"Palabra\",\n",
        "    x=\"Frecuencia\",\n",
        "    color=ColorIA\n",
        ")\n",
        "ax2.set_title(\"Top 20 unigramas ¬∑ IA (filtro extendido)\", fontsize=16, fontweight=\"bold\")\n",
        "ax2.set_xlabel(\"Frecuencia\", fontsize=12)\n",
        "ax2.set_ylabel(\"\")\n",
        "ax2.tick_params(labelsize=12)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rkfNuLjA17QY"
      },
      "source": [
        "En el gr√°fico comparativo de unigramas se aprecia con claridad que, aunque ambas clases tratan temas muy similares, no priorizan las mismas palabras.\n",
        "\n",
        "En los textos humanos el t√©rmino m√°s frecuente es ‚Äúcabello‚Äù, un sustantivo que apunta a un tema concreto; los siguientes puestos los ocupan palabras ligadas a la acci√≥n. En los textos de IA, en cambio, el t√©rmino que domina es ‚Äúhacer‚Äù, un verbo gen√©rico, seguido muy de cerca por ‚Äúembargo‚Äù, que es un conector discursivo.\n",
        "\n",
        "En s√≠ntesis: los textos humanos colocan primero el contenido (el tema), mientras que los textos de IA priorizan la forma o estructura del discurso.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "niED1Qg8rLns"
      },
      "source": [
        "## 1.6. An√°lisis de riqueza de vocabulario (TTR)\n",
        "\n",
        "El Type Token Ratio (TTR) es una m√©trica para estimar la riqueza l√©xica de un texto. Se calcula con la siguiente f√≥rmula:\n",
        "\n",
        "$$\n",
        "\\text{TTR} = \\frac{\\text{Num. palabras unicas (types)}}{\\text{Num. total palabras (tokens)}}\n",
        "$$\n",
        "\n",
        "Un TTR bajo indica que el texto reutiliza muchas palabras y por ende, tiene un estilo repetitivo. En cambio, un TTR alto es sin√≥nimo de un estilo m√°s variado ya que utiliza muchas palabras distintas.\n",
        "\n",
        "En este caso, se utiliza el filtrado b√°sico porque nos interesa medir la diversidad l√©xica de los textos, sin eliminar las muletillas caracter√≠sticas del corpus. Con el filtrado b√°sico se refleja mejor si los textos generados por IA tienden a reutilizar las mismas construcciones y si los textos humanos representan una mayor variedad l√©xica.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Km9KFAeIrTKD"
      },
      "outputs": [],
      "source": [
        "def CalcularTTR(texto):\n",
        "    \"\"\"Calcula el TTR de un texto\"\"\"\n",
        "    if not texto:\n",
        "        return 0\n",
        "    tokens = texto.split()\n",
        "    if not tokens:\n",
        "        return 0\n",
        "\n",
        "    num_tipos = len(set(tokens))\n",
        "    num_tokens = len(tokens)\n",
        "\n",
        "    return num_tipos / num_tokens\n",
        "\n",
        "# Aplicar funci√≥n a texto procesado b√°sico\n",
        "df['ttr'] = df['ContenidoBasico'].apply(CalcularTTR)\n",
        "\n",
        "# Visualizar la distribuci√≥n de TTR\n",
        "plt.figure()\n",
        "sns.kdeplot(data=df, x='ttr', hue='etiqueta', fill=True,\n",
        "            common_norm=False, palette=[ColorHumano, ColorIA])\n",
        "plt.title('Distribuci√≥n de Riqueza de Vocabulario (TTR) por Clase', fontsize=16, fontweight='bold')\n",
        "plt.xlabel('TTR (M√°s alto = M√°s diverso)', fontsize=12)\n",
        "plt.ylabel('Densidad', fontsize=12)\n",
        "plt.legend(title='Clase', labels=['1: IA', '0: Humano'])\n",
        "plt.show()\n",
        "\n",
        "# Mostrar estad√≠sticas de TTR\n",
        "print(\"\\nEstad√≠sticas de TTR por clase:\")\n",
        "print(df.groupby('etiqueta')['ttr'].describe())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2hcxgyU03ajJ"
      },
      "source": [
        "Ambas clases muestran valores elevados de TTR, ya que los textos tienen una longitud media y poca variedad de tem√°tica. La clase IA tiene una media ligeramente superior, pero no es  suficiente para usarlo como criterio de clasificaci√≥n.\n",
        "\n",
        "En conclusi√≥n, ninguna de las dos es claramente m√°s pobre l√©xicamente, la separaci√≥n entre las clases no est√° en la cantidad de vocabulario distinto sino en el vocabulario priorizado y en las estructuras que se repiten."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tcoSOzLQB7oI"
      },
      "source": [
        "##1.7. Conclusi√≥n general del EDA\n",
        "\n",
        "El EDA permite extraer varias ideas clave:\n",
        "\n",
        "  * **Longitud homog√©nea:** Los textos humanos y generados por IA tiene longitudes muy similares, por lo que el tama√±o del documento, por si solo no permite distinguir entre clases\n",
        "  \n",
        "  * **Corpus instructivo:** la gran mayor√≠a de textos responden a una misma intenci√≥n comunicativa y explicativa. Esto justifica la alta presencia de verbos modales y expresiones de procedimiento\n",
        "\n",
        "  * **Estilo diferente:**\n",
        "      * Los textos generados tienen una redacci√≥n m√°s de plantilla con conectores y f√≥rmulas de gu√≠as\n",
        "      * Los humanos incorporan m√°s referencias concretas, acciones espec√≠ficas y objetos\n",
        "  * **Necesidad del doble filtrado:** el filtrado extendido permite observar las diferencias reales entre clases ya que elimina el ruido y las muletillas del corpus\n",
        "En resumen, es un corpus homog√©neo en cuanto a tama√±o y con un estilo instructivo muy marcado. Lo que realmente distingue un texto de una clase de otra no es el tema, si no la forma de redacci√≥n. Por ello, los modelos que buscan patrones l√©xicos y de estilo tendran resultados m√°s adecuados."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ITx8nfpaCjhz"
      },
      "source": [
        "# **2. Modelos de Aprendizaje Autom√°tico Cl√°sico**\n",
        "\n",
        "En esta secci√≥n se entrenan y comparan clasificadores cl√°sicos aplicados al problema de detecci√≥n de texto generado por IA. El esquema seguido es el habitual en NLP tradicional:\n",
        "  1.   Utilizar un texto preprocesado. Se utiliza el filtrado b√°sico porque los vectorizadores de ` scikit-learn` son capaces de detectar que t√©rminos son demasiado frecuentes y reducirles el peso autom√°ticamente\n",
        "  2.   Vectorizar: convertir texto en representaci√≥n num√©rica\n",
        "  3.   Entrenar modelo de clasificaci√≥n\n",
        "\n",
        "Se evaluan 3 tipos de representaciones:\n",
        "\n",
        "  * **Bag of Words (BoW):** cuenta cuantas veces aparece cada t√©rmino en el documento.\n",
        "    * Funciona bien cuando el tema o el estilo diferencian las clases\n",
        "  * **TF-IDF:** penaliza las palabras muy frecuentes en todo el corpus y da m√°s peso a las que son espec√≠ficas del documento\n",
        "    * Es parecido, al filtrado extendido del apartado anterior (reducir ruido l√©xico)\n",
        "    * Suele ser una representaci√≥n robusta en textos cortos\n",
        "  * **Word2Vec (promediado):** proyecta cada palabra en un espacio denso de embeddings y despues promedia los vectores del documento\n",
        "    * El objetivo es capturar significado l√©xico, no solo frecuencia\n",
        "\n",
        "He entrenado las tres representaciones sobre 3 modelos cl√°sicos:\n",
        "  * **Regresi√≥n log√≠stica:** modelo lineal, r√°pido y con resultados estables\n",
        "  * **Naive Bayes:** modelo probabil√≠stico\n",
        "  * **SVM lineal:** m√°s potente cuando los datos est√°n bien representados\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9vWzBDx2Hl9"
      },
      "source": [
        "## 2.1. Divisi√≥n de datos\n",
        "\n",
        "Antes de entrenar un modelo, es necesario separar los datos en 2 subconjuntos:\n",
        "  * **Conjunto de entrenamiento (train):** son ejemplos que el modelo ve con ellos ajusta sus par√°metros y aprende\n",
        "  * **Conjunto de prueba (test):** el modelo no ve estos ejemplos por lo que se usan para evaluar su rendimiento real\n",
        "\n",
        "En este caso, se reserva el 20% de los textos para prueba."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "td36pOvpCFU2"
      },
      "outputs": [],
      "source": [
        "X = df['ContenidoBasico']\n",
        "y = df['etiqueta']\n",
        "\n",
        "# 80% train 20% test\n",
        "# random_state 42 para que la divisi√≥n sea aleatoria pero reproducible\n",
        "# stratify = y para que los conjuntos de train y test mantengan la misma proporci√≥n que el dataset original\n",
        "XTrain, XTest, YTrain, YTest = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "print(f\"Tama√±o de entrenamiento: {len(XTrain)} textos\")\n",
        "print(f\"Tama√±o de prueba: {len(XTest)} textos\\n\")\n",
        "\n",
        "print(\"Proporci√≥n de clases en Train:\")\n",
        "print(YTrain.value_counts(normalize=True), \"\\n\")\n",
        "\n",
        "print(\"Proporci√≥n de clases en Test:\")\n",
        "print(YTest.value_counts(normalize=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6mn1WN6eCIDG"
      },
      "source": [
        "##2.2. Funci√≥n Auxiliar de Evaluaci√≥n\n",
        "\n",
        "En la pr√°ctica hay que comparar 3 modelos y varias representaciones, es por eso que he hecho una √∫nica funci√≥n para centralizar la evaluaci√≥n. De esta forma, todos los clasificadores se miden con los mismos criterios y los resultados son facilmente comparables.\n",
        "\n",
        "\n",
        "**M√âTRICAS**\n",
        "\n",
        "En este caso, se utilizan 3 m√©tricas de evaluaci√≥n:\n",
        "\n",
        "  * **Accuracy:** Es la proporci√≥n de aciertos sobre el total. Aunque es una m√©trica intuitiva si las clases estan desbalanceadas puede dar un resultado m√°s positivo que la realidad\n",
        "\n",
        "  $$\\text{Accuracy} = \\frac{\\text{Num. predicciones correctas}}{\\text{Num. total predicciones}} $$\n",
        "\n",
        "  * **F1-Score (weighted):** combina precisi√≥n y recall en un solo valor. Esto es util si las clases no estan balanceadas\n",
        "    * Precisi√≥n: de los que se han predecido como IA cuantos lo son\n",
        "    * Recall: de los que son IA cuantos se han detectado\n",
        "\n",
        "  * **Informe de clasificaci√≥n:** permite ver la precisi√≥n, recall y F1 por clase lo que ayuda a ver si el modelo trata igual a ambas clases\n",
        "\n",
        "**MATRIZ DE CONFUSI√ìN**\n",
        "\n",
        "Es el informe de errores, ya que dice exactamente en qu√© se ha equivocado el modelo.\n",
        "\n",
        "  * **Verdaderos Positivos:** Era IA y dijo IA\n",
        "  * **Verdaderos Negativos:** Era Humano y dijo Humano\n",
        "  * **Falsos Positivos:** Era Humano pero dijo IA (Error Tipo I)\n",
        "  * **Falsos Negativos:** Era IA pero dijo Humano (Error Tipo II)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EH-VJk7tCK8M"
      },
      "outputs": [],
      "source": [
        "def EvaluarModelo(model, XTrainVec, YTrain, XTestVec, YTest, model_name, representation_name):\n",
        "    \"\"\"Entrena y eval√∫a un modelo, imprimiendo sus m√©tricas\"\"\"\n",
        "\n",
        "    print(f\"\\033[1mEvaluando: {model_name} con {representation_name}\\033[0m\")\n",
        "\n",
        "    # Entrenar\n",
        "    StartTime = time.time()\n",
        "    model.fit(XTrainVec, YTrain)\n",
        "    EndTime = time.time()\n",
        "    print(f\"  ¬∑ Tiempo de entrenamiento: {EndTime - StartTime:.2f} segundos\")\n",
        "\n",
        "    # Predecir\n",
        "    YPred = model.predict(XTestVec)\n",
        "\n",
        "    # Evaluar\n",
        "    accuracy = accuracy_score(YTest, YPred)\n",
        "    f1 = f1_score(YTest, YPred, average='weighted')\n",
        "\n",
        "    print(f\"  ¬∑ Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "    print(f\"  ¬∑ F1-Score (Weighted): {f1:.4f}\")\n",
        "\n",
        "    print(\"\\n  ¬∑ Informe de Clasificaci√≥n:\")\n",
        "    report = classification_report(YTest, YPred, target_names=['Humano', 'IA'])\n",
        "    print(\"    \" + report.replace(\"\\n\", \"\\n    \"))\n",
        "\n",
        "    # Matriz de Confusi√≥n\n",
        "    cm = confusion_matrix(YTest, YPred)\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='GnBu',\n",
        "                xticklabels=LabelNames,\n",
        "                yticklabels=LabelNames)\n",
        "    plt.title(f\"Matriz de Confusi√≥n - {model_name} ({representation_name})\")\n",
        "    plt.ylabel('Clase Real')\n",
        "    plt.xlabel('Clase Predicha')\n",
        "    plt.show()\n",
        "\n",
        "# Diccionario de modelos cl√°sicos\n",
        "models = {\n",
        "    \"Logistic Regression\": LogisticRegression(max_iter=1000, random_state=42),\n",
        "    \"Naive Bayes\": MultinomialNB(),\n",
        "    \"SVM (Lineal)\": SVC(kernel='linear', random_state=42)\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XtmPzgetCNf4"
      },
      "source": [
        "## 2.3. Comparativa de Representaciones y Modelos\n",
        "En los siguientes subapartados se aplican los 3 modelos cl√°sicos sobre las representaciones.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SD3uKVewGWBu"
      },
      "source": [
        "###2.3.1. Bolsa de palabras (BoW)\n",
        "Se construye un vocabulario con las palabras m√°s frecuentes. Cada documento se convierte en un vector donde cada posici√≥n indica cuantas veces aparece esa palabra. Aunque es muy simple y r√°pida no tiene en cuenta el orden ni la diferencia entre palabras comunes e informativas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-RZ4BXZACRtd"
      },
      "outputs": [],
      "source": [
        "# Vocabulario de m√°ximo 5.000 palabras\n",
        "# max_df=0.7: Ignora las muletillas\n",
        "VectorizarBoW = CountVectorizer(max_features=5000, min_df=5, max_df=0.7)\n",
        "\n",
        "XTrainBoW = VectorizarBoW.fit_transform(XTrain)\n",
        "XTestBoW = VectorizarBoW.transform(XTest)\n",
        "\n",
        "# Evaluar modelos con BoW\n",
        "for name, model in models.items():\n",
        "    EvaluarModelo(model, XTrainBoW, YTrain, XTestBoW, YTest, name, \"BoW\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qDHTuDjiv8_t"
      },
      "source": [
        "**An√°lisis de Resultados**\n",
        "\n",
        "Los tres modelos obtienen un rendimiento muy similar, con valores de accuracy cerca del 76%. Esto indica que incluso una representaci√≥n puramente frecuencial es capaz de capturar diferencias l√©xicas entre ambas clases. A√∫n as√≠, la matriz de confusi√≥n muestra que el n√∫mero de errores se mantiene en torno a las 200 instancias, lo que demuestra que BoW no filtra bien el ruido (estilo instructivo) que se detect√≥ en el EDA."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yi9OP76hCVDi"
      },
      "source": [
        "###2.3.2. TF-IDF\n",
        "\n",
        "El planteamiento principal es el mismo que el BoW pero este modelo pondera las palabras. Las palabras que aparecen en muchos documentos pierden peso mientras que las que son m√°s espec√≠ficas ganan peso.\n",
        "\n",
        "Esto es muy √∫til ya que como se usa el texto con el preprocesado b√°sico hay muchas palabras instructivas que no ayudan a distinguir.\n",
        "\n",
        "Adem√°s, se ha empleado el par√°metro `ngram_range=(1, 3)`. El an√°lisis exploratorio de N-gramas del EDA demostr√≥ que una parte importante de la caracter√≠stica diferenciadora no estaba en las palabras aisladas, sino en bigramas y trigramas caracter√≠sticos de los textos generados por IA (conectores, estructuras paso a paso, expresiones recurrentes). Al permitir que el vectorizador genere caracter√≠sticas para unigramas, bigramas y trigramas, el modelo puede incorporar directamente esas secuencias como atributos diferenciadores."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "36Xk_nsbCXza"
      },
      "outputs": [],
      "source": [
        "# Vocabulario de m√°ximo 5.000 palabras\n",
        "# max_df=0.7: Ignora las muletillas\n",
        "VectorizarTFDF = TfidfVectorizer(max_features=5000, min_df=5, max_df=0.7, ngram_range=(1, 3))\n",
        "\n",
        "XTrainFDF = VectorizarTFDF.fit_transform(XTrain)\n",
        "XTestTFDF = VectorizarTFDF.transform(XTest)\n",
        "\n",
        "# Evaluar modelos con TF-IDF\n",
        "for name, model in models.items():\n",
        "    EvaluarModelo(model, XTrainFDF, YTrain, XTestTFDF, YTest, name, \"TF-IDF\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BkeCQIg72Q2l"
      },
      "source": [
        "**An√°lisis de Resultados**\n",
        "\n",
        "Sustituir BoW por TF-IDF incrementa el accuracy entre 2-3 puntos porcentuales en los 3 modelos. La mejora es consistente y por ello se puede asumir que la mejora viene de la representaci√≥n y no del modelo. TF-IDF reduce el peso de las \"muletillas\" detectadas en el corpus y permite a los modelos centrarse en los t√©rminos distintivos para cada clase. Las matrices de confusi√≥n muestran que tanto el n√∫mero de falsos positivos, como negativos se reduce respecto al apartado anterior."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gKstWzJRCZz1"
      },
      "source": [
        "###2.3.3. Word2Vec\n",
        "  1. Entrenar modelo Word2Vec sobre los textos de entrenamiento para obtener vectores de palabras densos\n",
        "  2. Representar cada documento como la media de los vectores de sus palabras\n",
        "\n",
        "Esta representaci√≥n tiene como ventaja el poder capturar informaci√≥n sem√°ntica, pero el promediado hace que se pierda tanto el orden como parte del estilo instructivo recurrente que vimos en el EDA.\n",
        "\n",
        "Por este motivo, en este apartado solo se han probado 2 modelos sobre los vectores (se omite Naive Bayes). Los embeddings Word2Vec son vectores densos que pueden contener valores negativos y Naive Bayes no esta pensado para ello, por lo que el supuesto del modelo no se puede cumplir y los resultados no son interpretables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MVtGG1q4CdC7"
      },
      "outputs": [],
      "source": [
        "# Preparar sentencias\n",
        "SentenciasW2V = [word_tokenize(doc) for doc in XTrain]\n",
        "\n",
        "# Entrenar modelo\n",
        "W2VDim = 100\n",
        "W2VModel = Word2Vec(sentences=SentenciasW2V,\n",
        "                     vector_size=W2VDim,\n",
        "                     window=5,\n",
        "                     min_count=2,\n",
        "                     workers=4,\n",
        "                     epochs=10)\n",
        "\n",
        "\n",
        "# Precomputar vocabulario para no crearlo en cada llamada\n",
        "W2VVocab = set(W2VModel.wv.index_to_key)\n",
        "\n",
        "def VectorDocumentos(doc, model, Nfeatures, vocab):\n",
        "    tokens = word_tokenize(doc)\n",
        "    vec = np.zeros((Nfeatures,), dtype=\"float32\")\n",
        "    n = 0\n",
        "    for w in tokens:\n",
        "        if w in vocab:\n",
        "            vec += model.wv[w]\n",
        "            n += 1\n",
        "    if n > 0:\n",
        "        vec = vec / n\n",
        "    return vec\n",
        "\n",
        "# Transformar XTrain y XTest\n",
        "XTrainW2V = np.array([VectorDocumentos(doc, W2VModel, W2VDim, W2VVocab) for doc in XTrain])\n",
        "XTestW2V = np.array([VectorDocumentos(doc, W2VModel, W2VDim, W2VVocab) for doc in XTest])\n",
        "\n",
        "# Evaluar modelos\n",
        "models_w2v = {\n",
        "    \"Logistic Regression\": LogisticRegression(max_iter=1000, random_state=42),\n",
        "    \"SVM (RBF Kernel)\": SVC(kernel='rbf', random_state=42)\n",
        "}\n",
        "\n",
        "for name, model in models_w2v.items():\n",
        "    EvaluarModelo(model, XTrainW2V, YTrain, XTestW2V, YTest, name, \"Word2Vec\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jk9sTB0w4tBt"
      },
      "source": [
        "**An√°lisis de Resultados**\n",
        "\n",
        "En este √∫ltimo caso se observa que el redimiento de los 2 modelos disminuyen notablemente. El promedio de embeddings elimina informaci√≥n esencial sobre el orden y estilo que en este corpus es especialmente relevante ya que las estructuras instructivas son caracter√≠sticas de la clase IA.\n",
        "\n",
        "Al homogenizar el documento en un √∫nico vector, la representaci√≥n aproxima mucho los textos de ambas clases. El resultado es un aumento de error de los dos tipos. Aunque he intentado mejorar los resultados utilizando un SVM no lineal no logra alcanzar los resultados del apartado anterior."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RNgXaJB0Cou1"
      },
      "source": [
        "## 2.5. Conclusiones\n",
        "\n",
        "La comparativa confirma que la elecci√≥n de la representacion es el factor m√°s influyente en el rendimiento. Cambiar la representacion de BoW a TF-IDF mejora todos los clasificadores, lo que demuestra que el problema no estaba en el modelo sino en c√≥mo se codifica el texto.\n",
        "\n",
        "TF-IDF es la representaci√≥n m√°s adecuada para el preprocesado con el filtrado b√°sico, ya que reduce automaticamente el peso de los t√©rmino muy frecuentes y permite que el modelo evalue palabras y secuencias realmente discriminativas.\n",
        "\n",
        "Aunque la representaci√≥n tiene un papel fundamental en los resultados no es el √∫nico elemento relevante. Los 3 clasificadores utilizados tienen comportamientos diferenciados:\n",
        "\n",
        "  * **Regresi√≥n Log√≠stica y SVM lineal:** ambos modelos tienen resultados muy parecidos con TF-IDF\n",
        "\n",
        "  * **Naive Bayes:** aunque ofrece un rendimiento cercano a los dos modelos anteriores, la independencia condicional hace que para este Dataset en concreto favorezca m√°s a una de las clases (se observa facilmente en las matrices de confusi√≥n)\n",
        "\n",
        "Ademas de la accuracy, la m√©trica F1 confirma el comportamiento mencionado: en la mayor√≠a de casos F1 queda ligeramente por debajo del accuracy lo que demuestra que hay un peque√±o desbalance en la detecci√≥n. El informe de clasificaci√≥n refuerza este argumento ya que se puede ver que precisi√≥n y recall por cada clase no son id√©nticos.\n",
        "\n",
        "En cuanto al tiempo de entrenamiento, todos los modelos se ejecutaron en tiempos reducidos (cuesti√≥n de segundos), lo que permite explorar varias combinaciones de vectorizador y clasificador dentro del mismo flujo de trabajo. No obstante, esta eficiencia computacional no elimina el margen de error residual.\n",
        "\n",
        "Finalmente, las matrices de confusi√≥n siguen mostrando errores en ambos sentidos, lo que evidencia una limitaci√≥n inherente a los enfoques cl√°sicos: al basarse en vectores independientes del contexto, pierden parte de la informaci√≥n estructural y discursiva que tambi√©n distingue a las dos clases. Para reducir este margen de error ser√≠a necesario emplear modelos capaces de explotar contexto secuencial o dependencias a mayor longitud."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vyycWM3TS2H7"
      },
      "source": [
        "# **3. Modelado de t√≥picos (Topic Modeling)**\n",
        "\n",
        "En la secci√≥n anterior se trabaj√≥ con modelos supervisados: partiendo de textos etiquetados, el objetivo era aprender a predecir esa etiqueta.\n",
        "\n",
        "En este apartado el enfoque es distinto, porque se aplica aprendizaje no supervisado (agrupa seg√∫n su contenido l√©xico). La idea es explorar si en el corpus existen temas que ayuden a caracterizar los textos independientemente de la etiqueta.\n",
        "\n",
        "Este es el objetivo del modelado de t√≥picos, dado un conjunto de documentos, identificar grupos de palabras que suelen aparecer juntas y que pueden interpretarse como t√≥picos. La meta es descubrir estructura sem√°ntica en el conjunto de textos.\n",
        "\n",
        "En este trabajo se utilizan 2 modelos:\n",
        "\n",
        "- **Latent Dirichlet Allocation (LDA):** modelo probabil√≠stico generativo en el que es necesario fijar primero el n√∫mero de t√≥picos\n",
        "- **Hierarchical Dirichlet Process (HDP):** variante no param√©trica de LDA que intenta adivinar autom√°ticamente cu√°ntos t√≥picos son necesarios a partir de los datos\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z1DOT6EUYGWO"
      },
      "source": [
        "##3.1. Preparaci√≥n de datos para Gensim\n",
        "\n",
        "Los modelos de t√≥picos de Gensim no consumen un DataFrame con texto plano sino que necesitas 2 estructuras espec√≠ficas:\n",
        "  1. **Dictionary:** relaciona cada t√©rmino distinto del corpus con un identificador num√©rico\n",
        "  2.  **Corpus en formato BoW:** cada documento esta representado con una lista de tuplas (id, frecuencia)\n",
        "\n",
        "Como se explico en el apartado de preprocesamiento en este apartado se usa el filtrado b√°sico ya que es preferible conservar toda la variaci√≥n l√©xica posible porque la existencia de expresiones muy frecuentes pueden ser por si misma un t√≥pico de estilo. Por ejemplo, el tono instructivo es un t√≥pico que se relaciona con los textos creados por IA generativa.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1rE-k5eEClYT"
      },
      "outputs": [],
      "source": [
        "# Usar todos los textos preprocesados (del filtro b√°sico)\n",
        "TextTokens = [word_tokenize(doc) for doc in df['ContenidoBasico']]\n",
        "\n",
        "# Crear el Diccionario de Gensim\n",
        "dictionary = Dictionary(TextTokens)\n",
        "\n",
        "# Filtrar extremos\n",
        "  # Eliminar palabras que son  muy raras\n",
        "  # Eliminar \"stopwords\" (demasiado comunes)\n",
        "dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)\n",
        "\n",
        "# Crear el Corpus (Formato BoW)\n",
        "corpus = [dictionary.doc2bow(text) for text in TextTokens]\n",
        "\n",
        "print(f\"Diccionario creado con {len(dictionary)} palabras √∫nicas\")\n",
        "print(f\"Corpus creado con {len(corpus)} documentos\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vuNAo0PACiXj"
      },
      "source": [
        "Al aplicar el filtrado de extremos se reduce el vocabulario a 3122 palabras √∫nicas, l√©xico significativo. Este filtrado se basa en la Ley de Zipf: se eliminan las palabras que aparecen en menos de 15 documentos (`no_below=15`) y los que aparecen m√°s del 50% de todos los documentos (stopwords. `no_above=0.5`). Esto da como resultado un vocabulario limpio para que los modelos detecten temas y patrones de estilo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HvK8Y26QCvbM"
      },
      "source": [
        "\n",
        "### 3.2. Modelo LDA\n",
        "\n",
        "LDA es el modelo de t√≥picos m√°s utilizado, tanto por su interpretabilidad como porque suele ser relativamente estable.\n",
        "\n",
        "El modelo parte de dos hip√≥tesis b√°sicas:\n",
        "1. Cada documento est√° formado por una combinaci√≥n de t√≥picos\n",
        "2. Cada t√≥pico est√° formado por una combinaci√≥n de palabras con distintas probabilidades\n",
        "\n",
        "En este caso se han fijado 5 t√≥picos para facilitar la interpretaci√≥n de los resultados.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tpKtZSJSCy3s"
      },
      "outputs": [],
      "source": [
        "NtopicsLDA = 5\n",
        "\n",
        "# Entrenar con 10 passes (como epoch)\n",
        "lda_model = LdaModel(corpus=corpus,\n",
        "                     id2word=dictionary,\n",
        "                     num_topics=NtopicsLDA,\n",
        "                     random_state=42,\n",
        "                     passes=10,\n",
        "                     alpha='auto',\n",
        "                     per_word_topics=True)\n",
        "\n",
        "# Mostrar T√≥picos\n",
        "print(\"\\nT√≥picos encontrados por LDA:\")\n",
        "for idx, topic in lda_model.print_topics(-1):\n",
        "    print(f\"T√≥pico: {idx} \\nPalabras: {topic}\\n\")\n",
        "\n",
        "# M√©trica de Coherencia\n",
        "CoherenceLDA = CoherenceModel(model=lda_model, texts=TextTokens, dictionary=dictionary, coherence='c_v')\n",
        "CoherenceLDANum = CoherenceLDA.get_coherence()\n",
        "print(f\"Puntuaci√≥n de Coherencia LDA: {CoherenceLDANum:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c_kYocR_Dk2f"
      },
      "source": [
        "El modelo genera 5 t√≥picos facilmente identificables:\n",
        "\n",
        "  * T√≥pico 0: discurso general\n",
        "    * Act√∫a como t√≥pico de fondo\n",
        "  * T√≥pico 1: estilo instructivo\n",
        "    * Es un t√≥pico estil√≠stico\n",
        "    * Coincide con el patr√≥n de redacci√≥n observado en los textos de IA\n",
        "  * T√≥pico 2: cuidado personal\n",
        "  * T√≥pico 3: tecnolog√≠a / programaci√≥n\n",
        "  * T√≥pico 4:narrativa\n",
        "  \n",
        "La puntuaci√≥n de coherencia obtenida es ‚âà0.49 un valor decente para un corpus tan heterog√©neo. Aunque el resultado no es lo √∫nico importante, sino que los t√≥picos que ha encontrados se entienden (describen temas y estilos). Esto significa que el modelo ha encontrado patrones en el corpus incluido el t√≥pico de estilo instructivo que ya se ha mencionado anteriormente.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r5nuW-3-C1Gy"
      },
      "source": [
        "### 3.3. Modelo HDP\n",
        "\n",
        "El modelo HDP es una extensi√≥n de LDA, pero no necesita par√°metros. Su principal ventaja es que no es necesario fijar el n√∫mero de t√≥picos, el modelo va generando tantos como considera necearios dependiendo de los datos.\n",
        "\n",
        "En la pr√°ctica, aunque HDP produce m√°s temas suelen ser menos n√≠tidos que LDA.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BBbeYCQpC3fg"
      },
      "outputs": [],
      "source": [
        "  # Entrenar HDP\n",
        "  hdp_model = HdpModel(corpus=corpus, id2word=dictionary)\n",
        "\n",
        "  # Todos los t√≥picos generados por el modelo\n",
        "  NTopicsHDP = hdp_model.get_topics().shape[0]\n",
        "  print(f\"\\nT√≥picos generados por HDP: {NTopicsHDP}\")\n",
        "\n",
        "  # Mostrar los 5 m√°s relevantes\n",
        "  print(\"\\nTop 5 t√≥picos m√°s relevantes encontrados por HDP:\")\n",
        "  TopicsHDP = hdp_model.print_topics(num_topics=5, num_words=10)\n",
        "  for idx, topic in enumerate(TopicsHDP):\n",
        "      print(f\"\\nT√≥pico {idx}:\")\n",
        "      print(topic)\n",
        "\n",
        "  # Coherencia GLOBAL del modelo HDP (todas las topics que gener√≥)\n",
        "  CoherenceHDP = CoherenceModel(\n",
        "      model=hdp_model,\n",
        "      texts=TextTokens,\n",
        "      dictionary=dictionary,\n",
        "      coherence='c_v'\n",
        "  )\n",
        "  CoherenceHDPNum = CoherenceHDP.get_coherence()\n",
        "  print(f\"\\nCoherencia global HDP: {CoherenceHDPNum:.4f}\")\n",
        "\n",
        "  # Coherencia SOLO de los 5 primeros t√≥picos (para comparar con LDA de 5)\n",
        "  Top5Topics = [\n",
        "      [w for w, _ in hdp_model.show_topic(topic_id, topn=10)]\n",
        "      for topic_id in range(5)\n",
        "  ]\n",
        "\n",
        "  CoherenceHDP_top5 = CoherenceModel(\n",
        "      topics=Top5Topics,\n",
        "      texts=TextTokens,\n",
        "      dictionary=dictionary,\n",
        "      coherence='c_v'\n",
        "  )\n",
        "  CoherenceHDPT5Num = CoherenceHDP_top5.get_coherence()\n",
        "  print(f\"Coherencia HDP (solo top 5): {CoherenceHDPT5Num:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twQN7kmyKiaA"
      },
      "source": [
        "El modelo HDP genera 150 t√≥picos, un n√∫mero mucho m√°s grande que el necesario para describir este corpus. Los cinco t√≥picos m√°s relevantes estan dominados por el mismo bloque l√©xico pero con ligeras variaciones (estilo instructivo).\n",
        "\n",
        "Aunque tanto la coherencia global como de los cinco dominantes es superior a la del modelo anterior. El hecho de que la coherencia sea mayor se debe a que el modelo crea muchos t√≥picos peque√±os y muy homog√©neos pero el resultados es dif√≠cil de interpretar.\n",
        "\n",
        "Por este motivo en la secci√≥n de ejemplos se utiliza el modelo LDA."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gonuSOb7C5kQ"
      },
      "source": [
        "### 3.4. Relaci√≥n T√≥picos-Clases\n",
        "\n",
        "A pesar de que ninguno de estos modelos utilizan las etiquetas durante el entrenamiento, las etiquetas son muy √∫tiles para comprobar si ciertos temas aparecen m√°s en texto humanos o en textos IA. Para comprobarlo, se asigna a cada documento (unidad individual de texto que analiza el modelo, en el DataSet equivale a las filas de texto) su t√≥pico dominantes y se agrupa por clase.\n",
        "\n",
        "Esta relaci√≥n permite evaluar la utilidad de los t√≥picos en el contexto de este DataSet. En el apartado del EDA pudimos ver que muchos textos generados por IA siguen un estilo m√°s instructivo. Por tanto, la hip√≥tesis es que algunos t√≥picos se concentren m√°s en la clase IA, los m√°s instructivos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ih007QE5yZS4"
      },
      "source": [
        "###3.4.1 Distribuci√≥n de t√≥picos por clase: LDA\n",
        "\n",
        "En este subapartado se utiliza el modelo LDA con 5 t√≥picos. Se representa cu√°ntos textos de cada clase se relacionan con cada tema y se muestra una tabla de proporciones."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QyRzUgIKC8me"
      },
      "outputs": [],
      "source": [
        "if 'lda_model' in locals():\n",
        "    # Asignar t√≥pico principal a cada documento\n",
        "    LDADocTopics = [lda_model.get_document_topics(doc) for doc in corpus]\n",
        "\n",
        "    def ObtenerTopicPrincipal(topics):\n",
        "        \"\"\"Devuelve el ID del t√≥pico con mayor probabilidad para un documento\"\"\"\n",
        "        if not topics:\n",
        "            return None\n",
        "        main_topic = max(topics, key=lambda x: x[1])  # (topic_id, frecuencia)\n",
        "        return main_topic[0]\n",
        "\n",
        "    df[\"LDA_topic_id\"] = [ObtenerTopicPrincipal(t) for t in LDADocTopics]\n",
        "    df[\"LDA_topic_id\"] = df[\"LDA_topic_id\"].astype(\"Int64\")\n",
        "\n",
        "    # Distribuci√≥n de t√≥picos por clase\n",
        "    plt.figure(figsize=(12, 7))\n",
        "    sns.countplot(\n",
        "        data=df,\n",
        "        x=\"LDA_topic_id\",\n",
        "        hue=\"etiqueta\",\n",
        "        palette=[ColorHumano, ColorIA]\n",
        "    )\n",
        "    plt.title(\"Distribuci√≥n de t√≥picos LDA por clase\", fontsize=16, fontweight=\"bold\")\n",
        "    plt.xlabel(\"ID del t√≥pico (LDA)\", fontsize=12)\n",
        "    plt.ylabel(\"Cantidad de documentos\", fontsize=12)\n",
        "    plt.legend(title=\"Clase\", labels=LabelNames)\n",
        "    plt.show()\n",
        "\n",
        "    # Tabla de proporciones\n",
        "    DistribucionTopics = (\n",
        "        df.groupby(\"etiqueta\")[\"LDA_topic_id\"]\n",
        "          .value_counts(normalize=True)\n",
        "          .unstack(fill_value=0)\n",
        "    )\n",
        "    print(\"\\nProporci√≥n de t√≥picos por clase (0=Humano, 1=IA):\")\n",
        "    print(DistribucionTopics)\n",
        "else:\n",
        "    print(\"Modelo LDA no fue entrenado. Se omite el an√°lisis por t√≥picos.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VBLK59c3NMaQ"
      },
      "source": [
        "En esta secci√≥n se le asigna a cada documento un t√≥pico LDA (el que mayor probabilidad tenga). Esto permite comprobar si los estilos detectados coinciden con los patrones observados en el EDA.\n",
        "\n",
        "El t√≥pico 1, estilo instructivo, agrupa el 44.95% de los textos IA (casi la mitad) y solo el 21.05% de los textos humanos. Esto confirma que el modelo aisla correctamente el estilo caracter√≠sticos de la IA.\n",
        "\n",
        "El t√≥pico 2 trata sobre la belleza y el cuidado personal, aparece en el 24.41% de los textos humanos y solo en el 7.98% de los textos IA. Lo que sugiere que este tema es m√°s t√≠pico de la producci√≥n humana.\n",
        "\n",
        "Los otros 3 t√≥picos presentan distribuciones m√°s equilibradas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ZzbBcaD2583"
      },
      "source": [
        "###3.4.2 Distribuci√≥n de t√≥picos por clase: HDP\n",
        "\n",
        "En la secci√≥n 3.3 se puede observar que el modelo HDP genera 150 t√≥picos. Para hacer una comparaci√≥n razonable se ha limitado el an√°lisis a los 5 t√≥picos m√°s frecuentes y el resto de temas se agrupa en la categor√≠a \"Otros\". As√≠ se ve si en los t√≥picos principales hay una preferencia por clase. El m√©todo es igual seguido para el modelo LDA."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yXT5UQPmyrXx"
      },
      "outputs": [],
      "source": [
        "if 'hdp_model' in locals():\n",
        "\n",
        "  # Asignar t√≥pico principal a cada documento\n",
        "  HDPDocTopics = [hdp_model[doc] for doc in corpus]\n",
        "\n",
        "  def ObtenerTopicPrincipalHDP(topics):\n",
        "      \"\"\"Devuelve el ID del t√≥pico de HDP con mayor probabilidad.\"\"\"\n",
        "      if not topics:\n",
        "          return None\n",
        "      main_topic = max(topics, key=lambda x: x[1])\n",
        "      return int(main_topic[0])\n",
        "\n",
        "  df[\"HDP_topic_id\"] = pd.Series(\n",
        "      [ObtenerTopicPrincipalHDP(t) for t in HDPDocTopics]\n",
        "  ).astype(\"Int64\")\n",
        "\n",
        "  # Limitar a los 5 primeros\n",
        "  top_k = 5\n",
        "  df[\"HDP_topic_id_limited\"] = df[\"HDP_topic_id\"].apply(\n",
        "      lambda x: x if (pd.notna(x) and x < top_k) else top_k\n",
        "  )\n",
        "\n",
        "  # Distribuci√≥n de t√≥picos por clase\n",
        "  plt.figure(figsize=(12, 7))\n",
        "  sns.countplot(\n",
        "      data=df,\n",
        "      x=\"HDP_topic_id_limited\",\n",
        "      hue=\"etiqueta\",\n",
        "      palette=[ColorHumano, ColorIA],\n",
        "  )\n",
        "  plt.title(\"Distribuci√≥n de t√≥picos HDP por clase (Top 5 + Otros)\", fontsize=16, fontweight=\"bold\")\n",
        "  plt.xlabel(\"ID de t√≥pico (HDP)\", fontsize=12)\n",
        "  plt.ylabel(\"Cantidad de documentos\", fontsize=12)\n",
        "  plt.xticks(\n",
        "      ticks=range(top_k + 1),\n",
        "      labels=[str(i) for i in range(top_k)] + [\"Otros\"]\n",
        "  )\n",
        "  plt.legend(title=\"Clase\", labels=LabelNames)\n",
        "  plt.show()\n",
        "\n",
        "  # Tabla de proporciones\n",
        "  HDPDistTopics = (\n",
        "      df.groupby(\"etiqueta\")[\"HDP_topic_id_limited\"]\n",
        "        .value_counts(normalize=True)\n",
        "        .unstack(fill_value=0)\n",
        "  )\n",
        "  print(\"\\nProporci√≥n de t√≥picos HDP por clase (0=Humano, 1=IA):\")\n",
        "  print(HDPDistTopics)\n",
        "else:\n",
        "  print(\"Modelo HDP no fue entrenado. Se omite el an√°lisis por t√≥picos.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w2rW0NAYPk4G"
      },
      "source": [
        "Como HDP gener√≥ 150 t√≥picos, para la comparaci√≥n se agruparon todos los t√≥picos salvo los 5 m√°s frecuentes en un tema conjunto llamado \"Otros\". Sin embargo, la mayor parte de documentos quedaron asignados al t√≥pico 0 53% de los humanos y el 72% de la clase 1.\n",
        "\n",
        "Esto demuestra que el modelo no consigui√≥ separar ni los estilos ni los temas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PTxEDUQysxc2"
      },
      "source": [
        "## 3.5. Ejemplos representativos y discusi√≥n sem√°ntica\n",
        "\n",
        "En las secciones previas se han evaluado los modelos cuantitavimente, seg√∫n la m√©trica de coherencia y por su distribuci√≥n. Sin embargo, estas m√©tricas no garantizan que los t√≥picos creados por los modelos sean √∫tiles o interpretables. En este apartado se hace una inspecci√≥n cualitativa para validar la coherencia sem√°ntica.\n",
        "\n",
        "Un modelo LDA genera 2 tipos de distribuciones:\n",
        "  * Lista de temas: el modelo inventa t√≥picos y para cada uno de ellos indica cu√°les son las palabras m√°s t√≠picas de ese tema\n",
        "    * Indica c√≥mo es un t√≥pico\n",
        "  * Lista de textos: cada documento es una mezcla de t√≥picos con distintos pesos\n",
        "    * Indica  cu√°nto de cada tema tiene un texto\n",
        "\n",
        "La validaci√≥n consiste en cruzar ambas salidas: si las palabras que definen el t√≥pico coinciden con el contenido de los documentos en los que ese t√≥pico tiene mayor peso, ser√° sem√°nticamente coherente."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78HDy0xwvBpw"
      },
      "source": [
        "###3.5.1. Funciones de apoyo\n",
        "Para hacer la revisi√≥n de manera sistem√°tica se definen 2 funciones auxiliares:\n",
        "\n",
        "**TopicoGlobal**\n",
        "\n",
        "Muestra las palabras clave del t√≥pico y los `n` documentos del conjunto completo donde ese t√≥pico tiene mayor probabilidad. Es √∫til para entender de forma r√°pida la idea central del t√≥pico.\n",
        "\n",
        "**TopicoClase**\n",
        "\n",
        "Muestra las mismas palabras clave pero separa los ejemplos por clase, mostrando los `n` documentos m√°s representativos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QHYHMBsag8Rj"
      },
      "outputs": [],
      "source": [
        "def TopicoGlobal(topico_id: int, n: int = 3):\n",
        "    \"\"\"Muestra las palabras clave de un t√≥pico y los n documentos donde m√°s aparece.\"\"\"\n",
        "\n",
        "    # Mostrar las palabras del t√≥pico\n",
        "    try:\n",
        "        kw = \", \".join(w for w, _ in lda_model.show_topic(topico_id, topn=7))\n",
        "        print(f\"\\n\\033[1mT√≥pico {topico_id} ¬∑ Top-{n} global \\033[0m\")\n",
        "        print(f\"  Palabras clave: {kw}\\n\")\n",
        "    except Exception:\n",
        "        print(f\"\\n\\033[1mT√≥pico {topico_id} ¬∑ Top-{n} global \\033[0m \\n\")\n",
        "\n",
        "    # Buscar documentos donde ese t√≥pico tiene m√°s peso\n",
        "    filas = []\n",
        "    m = min(len(df), len(LDADocTopics))\n",
        "    for i in range(m):\n",
        "        p = 0.0\n",
        "        for t_id, pr in LDADocTopics[i]:\n",
        "            if t_id == topico_id:\n",
        "                p = float(pr)\n",
        "                break\n",
        "        if p > 0:\n",
        "            filas.append((p, i))\n",
        "\n",
        "    if not filas:\n",
        "        print(\"(No hay documentos con probabilidad > 0)\\n\")\n",
        "        return\n",
        "\n",
        "    # Ordenar por probabilidad y mostrar los N mejores\n",
        "    filas.sort(reverse=True)\n",
        "    for k, (p, idx) in enumerate(filas[:n], start=1):\n",
        "        if \"etiqueta\" in df.columns:\n",
        "            cls = \"IA\" if df.iloc[idx][\"etiqueta\"] == 1 else \"Humano\"\n",
        "        else:\n",
        "            cls = \"‚Äî\"\n",
        "\n",
        "        print(f\"  EJEMPLO #{k} ¬∑ Prob={p:.2%} ¬∑ Clase={cls}\")\n",
        "        print(f\"    {df.iloc[idx][\"contenido\"]}\")\n",
        "        print()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kq-fEms7r5Fb"
      },
      "outputs": [],
      "source": [
        "def TopicoClase(topico_id: int, n: int = 2):\n",
        "    \"\"\"Muestra los n documentos m√°s representativos de un t√≥pico separados por clase (0=Humano, 1=IA).\"\"\"\n",
        "    if \"etiqueta\" not in df.columns:\n",
        "        raise ValueError(\"Falta la columna 'etiqueta' (0/1) en el dataframe.\")\n",
        "\n",
        "    # Mostrar palabras del t√≥pico\n",
        "    try:\n",
        "        kw = \", \".join(w for w, _ in lda_model.show_topic(topico_id, topn=7))\n",
        "        print(f\"\\n\\033[1mT√≥pico {topico_id} ¬∑ Top-{n} por clase\\033[0m\")\n",
        "        print(f\"  Palabras clave: {kw}\\n\")\n",
        "    except Exception:\n",
        "        print(f\"\\n\\033[1mT√≥pico {topico_id} ¬∑ Top-{n} por clase\\033[0m \\n\")\n",
        "\n",
        "    # Separar por clase\n",
        "    filas_h = []\n",
        "    filas_ia = []\n",
        "    m = min(len(df), len(LDADocTopics))\n",
        "\n",
        "    for i in range(m):\n",
        "        prob = 0.0\n",
        "        for t_id, p in LDADocTopics[i]:\n",
        "            if t_id == topico_id:\n",
        "                prob = float(p)\n",
        "                break\n",
        "        if prob == 0:\n",
        "            continue\n",
        "\n",
        "        if df.iloc[i][\"etiqueta\"] == 1:\n",
        "            filas_ia.append((prob, i))\n",
        "        else:\n",
        "            filas_h.append((prob, i))\n",
        "\n",
        "    # Funci√≥n auxiliar para imprimir\n",
        "    def mostrar(nombre, filas):\n",
        "        print(nombre)\n",
        "        if not filas:\n",
        "            print(\"(No hay ejemplos)\\n\")\n",
        "            return\n",
        "        filas.sort(reverse=True)\n",
        "        for k, (p, idx) in enumerate(filas[:n], start=1):\n",
        "            print(f\"  EJEMPLO #{k} ¬∑ Prob={p:.2%}\")\n",
        "            print(f\"    {df.iloc[idx][\"contenido\"]}\")\n",
        "            print()\n",
        "\n",
        "    mostrar(\"Humano\", filas_h)\n",
        "    mostrar(\"IA\", filas_ia)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hngXZLxeg-nF"
      },
      "outputs": [],
      "source": [
        "TopicoGlobal(1, n=3)\n",
        "TopicoGlobal(2, n=3)\n",
        "print(\"----------------------------------------------------------------------\")\n",
        "TopicoClase(1, n=2)\n",
        "TopicoClase(2, n=2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0R5-f-0TUrI"
      },
      "source": [
        "Para la inspecci√≥n cualitativa tom√© los resultados del modelo LDA, ya que fueron los m√°s informativos y mostraron una separaci√≥n m√°s clara entre las clases a pesar de que la coherencia fuera ligeramente inferior.\n",
        "\n",
        "Dentro de LDA en este apartado analizo los t√≥picos 1 y 2 porque seg√∫n la distribuci√≥n por clase, son los que mayores diferencias entre textos de ambas clases.\n",
        "\n",
        "**T√≥pico 1**\n",
        "\n",
        "Palabras clave: \"puede\", \"ser\", \"c√≥mo\", \"importante\", \"primero\"\n",
        "\n",
        "Tema principal: estilo instructivo\n",
        "\n",
        "Los 3 documentos con mayor probabilidad pertenecen a la clase IA ya que presentan exactamente el mismo patr√≥n discursivo detectado en el EDA. Adem√°s los temas de los ejemplos son distintos (drogas, medidas de costura, c√≥mo usar leggings) lo que indica que lo que el t√≥pico captura no es un tema sino un estilo de redacci√≥n caracter√≠stico de la IA.\n",
        "\n",
        "\n",
        "**T√≥pico 2**\n",
        "\n",
        "Palabras clave: \"cabello\", \"agua\", \"piel\", \"usar\", \"aceite\"\n",
        "\n",
        "Tema principal: cuidado personal y belleza\n",
        "\n",
        "Estos documentos son m√°s directos y orientados a la acci√≥n, caracter√≠stica usual de los textos humanos,  lo que se confirma porque los documentos cuya probabilidad es mayor son escritos por humanos, como se puede observar en el \"Top Global\".\n",
        "\n",
        "\n",
        "En la distribuci√≥n de t√≥picos por clase se observa que, aunque ambos grupos aparecen con una probabilidad elevada en esos temas, la clase IA domina el T√≥pico 1, mientras que la clase Humano es m√°s frecuente en el T√≥pico 2."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3wT-_nMsW0Ah"
      },
      "source": [
        "#**4. Implementar una red LSTM**\n",
        "\n",
        "En el punto 2 se comprob√≥ que los modelos de clasificaci√≥n cl√°sicos, especialmente cuando se emple√≥ la representaci√≥n TF-IDF, alcanzaban un rendimiento satisfactorio. Sin embargo, estos modelos trabajan sobre representaciones de tipo bag of words, por lo que pierden por completo la informaci√≥n de orden y de estructura. En este corpus esa p√©rdida es especialmente relevante, porque el EDA mostr√≥ que una de las se√±ales m√°s claras para diferenciar textos humanos de textos generados por IA era precisamente el estilo secuencial e instructivo.\n",
        "\n",
        "El uso de Word2Vec tampoco solucion√≥ el problema: al promediar los embeddings se vuelve a destruir la informaci√≥n sobre la posici√≥n de las palabras en la secuencia.\n",
        "\n",
        "Teniendo en cuenta lo anterior, resulta natural recurrir a redes neuronales recurrentes y, en concreto, a una Long Short-Term Memory (LSTM), dise√±ada para mantener un estado de memoria y capturar dependencias a m√°s largo alcance, evitando las limitaciones de las RNN simples. Una LSTM procesa el texto palabra por palabra y mantiene dos estados internos, lo que le permite ‚Äúrecordar‚Äù lo que ha visto previamente. En este conjunto de datos esto es especialmente √∫til porque el modelo puede aprender que secuencias como ‚Äúen primer lugar‚Äù, ‚Äúsin embargo‚Äù o ‚Äúes importante‚Äù son indicativas del estilo propio de la IA. Adem√°s, se utiliza una LSTM bidireccional, que recorre la secuencia en ambos sentidos y, por tanto, interpreta cada palabra tambi√©n en funci√≥n del contexto posterior.\n",
        "\n",
        "Se analizan dos configuraciones:\n",
        "\n",
        "  * **LSTM con embeddings aprendidos desde cero:** la red inicializa la capa de embedding de forma aleatoria y aprende las representaciones durante el entrenamiento\n",
        "\n",
        "  * **LSTM con embeddings Word2Vec ajustables:** se cargan los vectores entrenados en el apartado *`2.3.3 Word2Vec`* y se permite el fine-tuning para adaptarlos a la tarea de clasificaci√≥n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wzEsMHL9W3g_"
      },
      "source": [
        "##4.1. Preparaci√≥n de Datos para Keras\n",
        "\n",
        "Las redes neuronales necesitan entradas num√©ricas y de longitud fija. Como en el apartado 2, en la LSTM utilizo el texto con el preprocesado b√°sico en el Tokenizer.\n",
        "\n",
        "Es verdad que el modelo de embeddings podr√≠a beneficiarse de ver el contexto completo, incluidas las palabras funcionales que se eliminan en ese filtrado, pero as√≠ mantengo un flujo coherente con el resto de la pr√°ctica, reutilizo c√≥digo y los resultados siguen siendo suficientemente buenos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Acvs2zO_W5-S"
      },
      "outputs": [],
      "source": [
        "MaxWords = 20000     # Max. palabras vocabulario\n",
        "max_length = 250     # Longitud m√°xima secuencias\n",
        "EmbeddingDim = 100   # Dimensi√≥n de los embeddings\n",
        "\n",
        "# Tokenizar\n",
        "tokenizer = Tokenizer(num_words=MaxWords, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(XTrain)\n",
        "\n",
        "# Diccionario (palabra, indice)\n",
        "WordIndex = tokenizer.word_index\n",
        "print(f\"Encontrados {len(WordIndex)} tokens √∫nicos.\")\n",
        "\n",
        "# Convertir textos a secuencias de enteros\n",
        "XTrainSeq = tokenizer.texts_to_sequences(XTrain)\n",
        "XTestSeq = tokenizer.texts_to_sequences(XTest)\n",
        "\n",
        "# Aplicar Padding\n",
        "  # Sirve para que todas las entradas tengan el mismo tama√±o (mejor para redes)\n",
        "XTrainpad = pad_sequences(XTrainSeq, maxlen=max_length, padding='post')\n",
        "XTestpad = pad_sequences(XTestSeq, maxlen=max_length, padding='post')\n",
        "\n",
        "print(f\"Forma de los datos de train (padded): {XTrainpad.shape}\")\n",
        "print(f\"Forma de los datos de test (padded): {XTestpad.shape}\")\n",
        "\n",
        "# Convertir etiquetas a array (requisito Keras)\n",
        "YTrain_arr = np.array(YTrain)\n",
        "YTest_arr = np.array(YTest)\n",
        "\n",
        "# Callback (si no mejora, para)\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zpbW6xW9W6i7"
      },
      "source": [
        "##4.2. Experimento 1: LSTM con Embeddings de la Red\n",
        "\n",
        "En este experimento se entrena una LSTM partiendo de embeddings aleatorios (la red los va aprendiendo). He hecho distintas versiones del modelo, la primera versi√≥n funcionaba pero empezaba a sobreajustar a partir de la 3.¬™ epoch, el accuracy del entrenamiento sub√≠a por encima de 0.9 mientras que la validaci√≥n se quedaba alrededor de 0.78.\n",
        "\n",
        "Para reducir el sobreajuste se hicieron 3 cambios peque√±os:\n",
        "\n",
        "  * LSTM devuelve la secuencia completa y a√±ade un `GlobalMaxPooling1D`\n",
        "    * El modelo recoge patrones de estilo que aparecen a lo largo de todo el texto\n",
        "\n",
        "  * Dejar m√°s regularizaci√≥n: se aumento el dropout en embedding y se a√±adi√≥n un dropout en la parte densa para que no memorice tan r√°pido"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X7_KPR4RW-OB"
      },
      "outputs": [],
      "source": [
        "ModelLSTM_1 = Sequential([\n",
        "    # Embeddings aprendidos desde cero\n",
        "    Embedding(input_dim=MaxWords, output_dim=EmbeddingDim, trainable=True), # trainable=True es el default\n",
        "\n",
        "    # Regularizar la entrada\n",
        "    SpatialDropout1D(0.2),\n",
        "\n",
        "    # LSTM bidireccional\n",
        "    Bidirectional(\n",
        "        LSTM(\n",
        "            units=80,\n",
        "            dropout=0.25,\n",
        "            return_sequences=True\n",
        "        )\n",
        "    ),\n",
        "\n",
        "    # Pooling temporal (activaciones m√°s fuertes)\n",
        "    GlobalMaxPooling1D(),\n",
        "\n",
        "    # Capa densa\n",
        "    Dense(\n",
        "        80,\n",
        "        activation='relu'\n",
        "    ),\n",
        "    Dropout(0.2),\n",
        "\n",
        "    # Salida binaria\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compilar el modelo\n",
        "ModelLSTM_1.compile(\n",
        "    optimizer='adam',\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Entrenar el modelo\n",
        "History1 = ModelLSTM_1.fit(\n",
        "    XTrainpad, YTrain_arr,\n",
        "    epochs=7,\n",
        "    batch_size=64,\n",
        "    validation_data=(XTestpad, YTest_arr),\n",
        "    callbacks=[early_stopping]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SIyUe6FFZEnu"
      },
      "source": [
        "## 4.3. Experimento 2: LSTM con *embeddings* Word2Vec (ajustables)\n",
        "\n",
        "En este segundo experimento se reutiliza el modelo Word2Vec entrenado en el apartado 2.3.3. En lugar de inicializar la capa `Embedding` con pesos aleatorios (como en el experimento 1), se cargan los vectores de Word2Vec obtenidos a partir del propio corpus. Esto implica aplicar **transfer learning**: partimos de una representaci√≥n que ya conoce el vocabulario principal y sus relaciones sem√°nticas, y dejamos que la red la refine para la tarea concreta de detecci√≥n Humano/IA.\n",
        "\n",
        "El flujo seguido es:\n",
        "\n",
        "* **Construcci√≥n de la matriz de *embeddings***\n",
        "\n",
        "  * Se crea una matriz de tama√±o `(MaxWords, EmbeddingDim)`.\n",
        "  * Para cada palabra del vocabulario del *tokenizer* que tambi√©n exista en el modelo Word2Vec se copia su vector en la posici√≥n correspondiente.\n",
        "  * Las palabras que no est√©n en Word2Vec quedan inicializadas a cero.\n",
        "\n",
        "* **Modelo LSTM bidireccional**\n",
        "\n",
        "  * Sobre esa capa de *embedding* inicializada se aplica la misma arquitectura que en el experimento 1: `SpatialDropout1D` ‚Üí LSTM bidireccional con `return_sequences=True` ‚Üí `GlobalMaxPooling1D` ‚Üí capas densas de clasificaci√≥n.\n",
        "  * Esto garantiza que la comparaci√≥n entre experimento 1 y 2 sea justa: cambia la inicializaci√≥n, no la arquitectura.\n",
        "\n",
        "* **Fine-tuning de los *embeddings***\n",
        "\n",
        "  * La capa `Embedding` se deja con `trainable=True`, de modo que la LSTM puede ‚Äúmover‚Äù ligeramente los vectores de Word2Vec para que reflejen mejor las diferencias de estilo detectadas en el EDA (tono instructivo, uso de conectores, f√≥rmulas t√≠picas de IA).\n",
        "\n",
        "**Nota importante**\n",
        "\n",
        "En versiones anteriores del cuaderno prob√© configuraciones donde el modelo Word2Vec y el Tokenizer de Keras ve√≠an textos ligeramente distintos.\n",
        "\n",
        "En la versi√≥n final, ambos se ajustan sobre el mismo preprocesado b√°sico, de modo que no existe desajuste de vocabulario entre los embeddings y las secuencias de entrada. Esto simplifica el flujo de trabajo y, aun sacrificando parte del contexto funcional, los resultados siguen siendo competitivos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mj9gNmcQZFXL"
      },
      "outputs": [],
      "source": [
        "# Construir la matriz de embeddings a partir del Word2Vec entrenado\n",
        "embedding_matrix = np.zeros((MaxWords, EmbeddingDim))\n",
        "for word, i in WordIndex.items():\n",
        "    # Solo palabras dentro del vocabulario\n",
        "    if i < MaxWords and word in W2VModel.wv:\n",
        "        embedding_matrix[i] = W2VModel.wv[word]\n",
        "\n",
        "# Definir el modelo LSTM usando embeddings\n",
        "ModelLSTM_2 = Sequential([\n",
        "    # Capa de Embedding inicializada con Word2Vec\n",
        "    Embedding(\n",
        "        input_dim=MaxWords,\n",
        "        output_dim=EmbeddingDim,\n",
        "        weights=[embedding_matrix],\n",
        "        trainable=True\n",
        "    ),\n",
        "\n",
        "    # Regularizar la entrada\n",
        "    SpatialDropout1D(0.2),\n",
        "\n",
        "    # LSTM bidireccional\n",
        "    Bidirectional(\n",
        "        LSTM(\n",
        "            units=80,\n",
        "            dropout=0.25,\n",
        "            return_sequences=True\n",
        "        )\n",
        "    ),\n",
        "\n",
        "    # Pooling temporal\n",
        "    GlobalMaxPooling1D(),\n",
        "\n",
        "    # Capa densa\n",
        "    Dense(\n",
        "        80,\n",
        "        activation='relu',\n",
        "    ),\n",
        "    Dropout(0.2),\n",
        "\n",
        "    # Salida binaria\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compilar el modelo\n",
        "ModelLSTM_2.compile(\n",
        "    optimizer='adam',\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Entrenar con las mismas condiciones que el experimento 1\n",
        "History2 = ModelLSTM_2.fit(\n",
        "    XTrainpad, YTrain_arr,\n",
        "    epochs=7,\n",
        "    # Como se ha implementado early stopping subir n√∫mero de epochs para dar margen\n",
        "    batch_size=64,\n",
        "    validation_data=(XTestpad, YTest_arr),\n",
        "    callbacks=[early_stopping]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LrOamcme0nQ4"
      },
      "source": [
        "##4.4. Evaluaci√≥n de las LSTM mediante m√©tricas de clasificaci√≥n\n",
        "\n",
        "Para comparar las LSTM con los modelos cl√°sicos no es suficiente con el accuracy: en el apartado 2 se analizaban tambi√©n F1, informe de clasificaci√≥n y matriz de confusi√≥n.\n",
        "\n",
        "En este apartado, se hace lo mismo para las dos variantes de LSTM para ver si realmente mejoran."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PdSF33Lb0786"
      },
      "source": [
        "### 4.4.1. Evaluaci√≥n Experimento 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "44wwj0jn0__s"
      },
      "outputs": [],
      "source": [
        "# Evaluaci√≥n test accuracy\n",
        "TestLoss, TestAcc = ModelLSTM_1.evaluate(XTestpad, YTest_arr, verbose=0)\n",
        "print(f\"Loss en test: {TestLoss:.4f}\")\n",
        "print(f\"Accuracy en test: {TestAcc:.4f}\")\n",
        "\n",
        "# Predicciones del modelo\n",
        "YPred_prob = ModelLSTM_1.predict(XTestpad, verbose=0)\n",
        "YPred = (YPred_prob >= 0.5).astype(int).ravel()\n",
        "\n",
        "\n",
        "print(\"\\nInforme de clasificaci√≥n ¬∑ LSTM E1\")\n",
        "print(classification_report(YTest_arr, YPred, target_names=LabelNames))\n",
        "\n",
        "# Matriz de confusi√≥n\n",
        "cm = confusion_matrix(YTest_arr, YPred)\n",
        "\n",
        "plt.figure(figsize=(5, 4))\n",
        "sns.heatmap(\n",
        "    cm,\n",
        "    annot=True,\n",
        "    fmt=\"d\",\n",
        "    cmap=\"GnBu\",\n",
        "    xticklabels=LabelNames,\n",
        "    yticklabels=LabelNames\n",
        ")\n",
        "plt.title(\"Matriz de confusi√≥n - LSTM Experimento 1 (Embeddings de la red)\")\n",
        "plt.xlabel(\"Clase predicha\")\n",
        "plt.ylabel(\"Clase real\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XS2duawy49El"
      },
      "source": [
        "Los resultados muestran que el modelo alcanza un buen rendimiento y sobre todo se mantiene un equilibrio entre las dos clases ya que el F1 es parecido para ambas clases: 0.79. La matriz de confusi√≥n muestra que hay m√°s error en la clase IA, se clasifican como humanos 199 documentos de clase 1. Esto sugiere que aunque aprenda los embeddings desde cero la LSTM es capaz de explotar los patrones secuenciales que detecte en el EDA y por eso se reducen los falsos negativos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6tH2-BKx1FDo"
      },
      "source": [
        "### 4.4.2. Evaluaci√≥n Experimento 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VKWAfNcM3Hhg"
      },
      "outputs": [],
      "source": [
        "# Evaluaci√≥n test accuracy\n",
        "TestLoss_2, TestAcc_2 = ModelLSTM_2.evaluate(XTestpad, YTest_arr, verbose=0)\n",
        "print(f\"Loss en test (W2V): {TestLoss_2:.4f}\")\n",
        "print(f\"Accuracy en test (W2V): {TestAcc_2:.4f}\")\n",
        "\n",
        "# Predicciones del modelo\n",
        "YPred_prob_2 = ModelLSTM_2.predict(XTestpad, verbose=0)\n",
        "YPred_2 = (YPred_prob_2 >= 0.5).astype(int).ravel()\n",
        "\n",
        "# M√©tricas de clasificaci√≥n\n",
        "print(\"\\nInforme de clasificaci√≥n ¬∑ LSTM E2 (W2V ajustable)\")\n",
        "print(classification_report(YTest_arr, YPred_2, target_names=LabelNames))\n",
        "\n",
        "# Matriz de confusi√≥n\n",
        "cm_2 = confusion_matrix(YTest_arr, YPred_2)\n",
        "\n",
        "plt.figure(figsize=(5, 4))\n",
        "sns.heatmap(\n",
        "    cm_2,\n",
        "    annot=True,\n",
        "    fmt=\"d\",\n",
        "    cmap=\"GnBu\",\n",
        "    xticklabels=LabelNames,\n",
        "    yticklabels=LabelNames\n",
        ")\n",
        "plt.title(\"Matriz de confusi√≥n - LSTM Experimento 2 (W2V ajustable)\")\n",
        "plt.xlabel(\"Clase predicha\")\n",
        "plt.ylabel(\"Clase real\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LxrwRls16SFz"
      },
      "source": [
        "En este experimento se observa una mejora consistente en todas las m√©tricas:\n",
        "  * **Loss:** baja hasta 0.38\n",
        "  * **Accuracy:** sube hasta 0.82\n",
        "  * **F1:** para ambas clases ~0.82\n",
        "  * **Matriz de confusi√≥n:** disminuyen tanto los falsos positivos como los negativos.\n",
        "\n",
        "Esto indica que iniciar con embeddings Word2Vec y permitir su ajuste permite dedicar m√°s capacidad a la se√±al de estilo. Converge mejor y tiene menos confusiones en las dos clases."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yIxDU8rSZQYs"
      },
      "source": [
        "## 4.5. Comparativa y justificaci√≥n de diferencias\n",
        "\n",
        "Tras entrenar los dos modelos LSTM y compararlos con los modelos cl√°sicos del apartado 2, se pueden extraer conclusiones claras sobre la ventaja de modelar secuencia y el efecto de usar *embeddings* Word2Vec ajustables."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HMeV5o1M2i2-"
      },
      "source": [
        "###4.5.1 Tabla comparativa de resultados\n",
        "\n",
        "Esta tabla muestra de manera muy visual los resultados obtenido en cada modelo y facilita la comparaci√≥n."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oYIOgmfJ2cAW"
      },
      "outputs": [],
      "source": [
        "data = {\n",
        "    \"Modelo\": [\n",
        "        \"SVM lineal (TF-IDF)\",\n",
        "        \"LSTM Exp. 1 (embeddings de cero)\",\n",
        "        \"LSTM Exp. 2 (W2V ajustable)\"\n",
        "    ],\n",
        "    \"Accuracy (test)\": [0.778, 0.79, 0.81],\n",
        "    \"F1 Humano\": [0.76, 0.78, 0.81],\n",
        "    \"F1 IA\": [0.78, 0.80, 0.83],\n",
        "    \"Falsos negativos\": [222, 199, 169],\n",
        "    \"Falsos positivos\": [182, 180, 159],\n",
        "}\n",
        "\n",
        "df_results = pd.DataFrame(data)\n",
        "df_results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PDpDjYhsQftU"
      },
      "source": [
        "### 4.5.1. LSTM vs. modelos cl√°sicos\n",
        "\n",
        "Incluso tomando como comparativa el peor LSTM (Experimento 1) y el mejor modelo cl√°sico con TF-IDF, la LSTM supera 0.79 frente a 0.77. √Ådemas, el n√∫mero de falsos negativos (error m√°s cr√≠tico IA clasificada como Humano) baja de 222 a 199.\n",
        "\n",
        "**Justificaci√≥n**\n",
        "\n",
        "Esto se debe  a 4 razones principales:\n",
        "\n",
        "Los modelos cl√°sicos trabajan sobre representaciones de BoW y aunque son eficaces no tienen en cuenta ni el orden ni la estructura.\n",
        "\n",
        "En este Dataset en concreto, el √°nalisis del apartado 1 mostr√≥ que la diferencia entre humano e IA radicaba en los patrones secuenciales y explicativos no solo en el vocabulario.\n",
        "\n",
        "La LSTM bidireccional procesa el texto palabra por palabra y recuerda el contexto anterior y posterior. Esto le permite detectar las plantillas de la IA.\n",
        "\n",
        "La capa `GlobalMaxPooling1D` act√∫a como un detector, esto quiere decir que si en cualquier parte del documento aparece una secuencia t√≠pica, la red la guarda y la utiliza para tomar la decisi√≥n final.\n",
        "\n",
        "**Resultados**\n",
        "\n",
        "Como resultado, se obtienen menos texto generados por IA clasificados como escritos por humanos, lo cual resuelve la limitaci√≥n principal de los modelos cl√°sicos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_OsERKkt5q6y"
      },
      "source": [
        "### 4.5.2. Experimento 1 vs. Experimento 2\n",
        "\n",
        "El modelo con embedding Word2Vec ajustables (Experimento 2) mejora el modelo que aprende los embeddings desde cero (Experimento 1) tanto en los falsos negativos como en los positivos.\n",
        "\n",
        "**Justificaci√≥n**\n",
        "\n",
        "La mejora de accuracy de casi 3 puntos se debe principalmente a 3 razones:\n",
        "\n",
        "En el primer experimento la red no solo deb√≠a aprender el significado de las palabras sino que tambien deb√≠a aprender a clasificar.\n",
        "\n",
        "En el segundo, a la red se le aplica transfer learning , la red inicia con una representaci√≥n sem√°ntica razonable. Adem√°s se permite el fine-tuning por lo que la red no debe perder el tiempo reaprendiendo sem√°nticamente sino ajustar la parte discriminativa.\n",
        "\n",
        "**Resultados**\n",
        "\n",
        "El segundo modelo es m√°s eficiente y equilibrado, clasifica mejor, por lo que reduce la confusi√≥n entre ambas clases."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sgm_c2-l-WEe"
      },
      "source": [
        "#**5. Fine-tuning de Modelos Transformer**\n",
        "\n",
        "Hasta ahora se ha trabajado con modelos entrenados sobre el propio corpus. Los modelos como LSTM son muy potentes pero su \"memoria\" es la limitaci√≥n principal, les es dif√≠cil relacionar palabras distantes y el procesamiento es lento porque tiene que analizar las palabras una a una.\n",
        "\n",
        "\n",
        "En cambio, en este apartado, se introduce un enfoque distinto: reutilizar un modelo de lenguaje preentrenado con millones de documentos (Transformers) y adaptarlo a la tarea de detecci√≥n de textos generados por IA.\n",
        "\n",
        "Los Transformers estudian todas las palabras a la vez y deciden cu√°les son importantes para entender el contexto de una palabra en particular, esto soluciona la limitaci√≥n de los modelos del apartado 4.\n",
        "\n",
        "El modelo BERT (modelo transformer) tiene 2 grandes ventajas para esta pr√°ctica en concreto:\n",
        "\n",
        "  * **Atenci√≥n especial a la secuencia:**\n",
        "    * Los modelos transformer pueden atender a todas las posiciones a la vez, no necesitan procesar el texto palabra por palabra\n",
        "      * Facilita capturar estructuras discursivas\n",
        "          * En el EDA se clasificaron como se√±ales IA\n",
        "  * **Representaciones contextuales:**\n",
        "    * El vector de una palabra depende de su contexto\n",
        "\n",
        "Como entrenar un Transformer desde cero no es viable, se carga un modelo preentrenado y se usa fine-tuning para adaptar el modelo a la tarea.\n",
        "\n",
        "Se realizan 2 experimentos:\n",
        "\n",
        "  * Fine-tuning ligero: se congela el modelo base y solo se entrena la capa de clasificaci√≥n\n",
        "  * Fine-tuning completo: el entrenamiento se hace en todas las capas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z2znyYy5-kQM"
      },
      "source": [
        "##5.1. Preparaci√≥n de Datos y Tokenizador (Hugging Face)\n",
        "\n",
        "Para trabajar con un modelo BERT preentrenado es necesario usar el mismo tokenizador con el qe se entren√≥ el modelo. Por eso, en este apartado se utiliza el Tokenize de la librer√≠a transformers (Hugging Face), lo que garantiza que las secuencias de entrada est√°n codificadas con el vocabulario correcto.\n",
        "\n",
        "Ademas, se convierte el DataFrame en un objeto de Hugging Face porque facilita la separaci√≥n, la tokenizaci√≥n por lotes y la integraci√≥n con el Trainer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MZNjOe4Z-n_-"
      },
      "outputs": [],
      "source": [
        "# Definir modelo base\n",
        "ModelName = \"bert-base-multilingual-cased\"\n",
        "\n",
        "# Cargar el Tokenizador\n",
        "tokenizer = AutoTokenizer.from_pretrained(ModelName)\n",
        "\n",
        "# Cargar y preparar el Dataset\n",
        " # Convertir  DataFrame de pandas a Dataset de Hugging Face\n",
        "df_hf = df.rename(columns={\"etiqueta\": \"label\"})\n",
        "hf_dataset = Dataset.from_pandas(df_hf)\n",
        "hf_dataset = hf_dataset.cast_column(\n",
        "    \"label\", ClassLabel(num_classes=2, names=[\"Humano\", \"IA\"])\n",
        ")\n",
        "\n",
        "# Separar train/test (Stratified)\n",
        "split = hf_dataset.train_test_split(\n",
        "    test_size=0.2,\n",
        "    seed=42,\n",
        "    stratify_by_column=\"label\"\n",
        ")\n",
        "DatasetTrain = split[\"train\"]\n",
        "DatasetTest = split[\"test\"]\n",
        "\n",
        "\n",
        "# Tokenizador\n",
        "def FuncionTokenizar(examples):\n",
        "    # Truncar a 256 la mayor√≠a de textos son cortos (EDA)\n",
        "    return tokenizer(\n",
        "        examples[\"contenido\"],\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=256,\n",
        "    )\n",
        "\n",
        "# Aplicar tokenizaci√≥n por lotes\n",
        "tokenizedTrain = DatasetTrain.map(FuncionTokenizar, batched=True)\n",
        "tokenizedTest  = DatasetTest.map(FuncionTokenizar, batched=True)\n",
        "\n",
        "# M√©tricas para el Trainer\n",
        "  # Mismas que resto de pr√°ctica\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    preds = np.argmax(logits, axis=-1)\n",
        "\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
        "        labels, preds, average=\"weighted\"\n",
        "    )\n",
        "    acc = accuracy_score(labels, preds)\n",
        "\n",
        "    return {\n",
        "        \"accuracy\": acc,\n",
        "        \"f1\": f1,\n",
        "        \"precision\": precision,\n",
        "        \"recall\": recall,\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_B9HNsqaAnjT"
      },
      "source": [
        "##5.2. Experimento 1: Fine-tuning Ligero (modelo congelado)\n",
        "\n",
        "El objetivo de este apartado es medir cuanto mejora el rendimiento del modelo solo usando representaciones contextuales de BERT (sin modificar el modelo base).\n",
        "\n",
        "El experimento consiste en congelar los pesos del modelo BERT base. Solo se entrenan los pesos de la capa de clasificaci√≥n. Por este motivo es r√°pido y computacionalmente barato (se optimizan al corpus pocos par√°metros frente a los 170 millones que tiene el modelo).\n",
        "\n",
        "La primera vez que entrene este modelo, congele todo el modelo y solo entrenaba la capa de clasificaci√≥n, pero el accuracy era muy bajo, por eso, decid√≠ hacer dos subapartados, uno entrenando solo la √∫ltima capa y otro dejando un poco m√°s de capacidad descongelando la √∫ltima capa de BERT."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###5.2.1. Experimento 1.1 - Modelo totalmente congelado"
      ],
      "metadata": {
        "id": "vWtvYm7wUA53"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "67TYxxRVAqwC"
      },
      "outputs": [],
      "source": [
        "print(\"\\n\\033[1mExperimento 1.1: Fine-tuning ligero (BERT totalmente congelado)\\033[0m\")\n",
        "\n",
        "# Cargar modelo base\n",
        "ModelLight_1 = AutoModelForSequenceClassification.from_pretrained(\n",
        "    ModelName,\n",
        "    num_labels=2\n",
        ")\n",
        "\n",
        "# Congelar todo el modelo base\n",
        "  #solo aprende  capa de clasificaci√≥n\n",
        "for p in ModelLight_1.base_model.parameters():\n",
        "    p.requires_grad = False\n",
        "\n",
        "# Argumentos de entrenamiento\n",
        "TrainingArgsLight_1 = TrainingArguments(\n",
        "    output_dir=\"./ResultsLight_1\",\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"f1\",\n",
        "    report_to=\"none\",\n",
        "    logging_dir=\"./logs_light_1\",\n",
        "    logging_steps=100,\n",
        ")\n",
        "\n",
        "# Trainer\n",
        "trainer_light_1 = Trainer(\n",
        "    model=ModelLight_1,\n",
        "    args=TrainingArgsLight_1,\n",
        "    train_dataset=tokenizedTrain,\n",
        "    eval_dataset=tokenizedTest,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "# Entrenar modelo y medir tiempo\n",
        "start = time.time()\n",
        "trainer_light_1.train()\n",
        "TimeLight_1 = time.time() - start\n",
        "TimeLight_1_min = TimeLight_1 / 60\n",
        "\n",
        "print(f\"Tiempo de entrenamiento (completo): {TimeLight_1:.2f} s ({TimeLight_1_min:.2f} min)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###5.2.1.1. Evaluaci√≥n y an√°lisis Experimento 1.1"
      ],
      "metadata": {
        "id": "59dsq4Q6gOGD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\\033[1mEvaluaci√≥n Experimento 1.1 (BERT congelado 100%)\\033[0m\")\n",
        "ResultsLight_1 = trainer_light_1.evaluate()\n",
        "print(ResultsLight_1)\n",
        "\n",
        "PredsLight_1 = trainer_light_1.predict(tokenizedTest)\n",
        "y_true_1 = PredsLight_1.label_ids\n",
        "YPred_1 = PredsLight_1.predictions.argmax(axis=1)\n",
        "\n",
        "print(\"\\n\\033[1mInforme de clasificaci√≥n Experimento 1.1 (BERT congelado 100%)\\033[0m\")\n",
        "print(classification_report(y_true_1, YPred_1, target_names=LabelNames))\n",
        "\n",
        "cm_1 = confusion_matrix(y_true_1, YPred_1)\n",
        "plt.figure(figsize=(4.5, 4))\n",
        "sns.heatmap(cm_1, annot=True, fmt=\"d\", cmap=\"GnBu\",\n",
        "            xticklabels=LabelNames, yticklabels=LabelNames)\n",
        "plt.title(\"Matriz de confusi√≥n ‚Äì Experimento 1.1 (BERT congelado 100%)\")\n",
        "plt.xlabel(\"Clase predicha\")\n",
        "plt.ylabel(\"Clase real\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TF73loZHWucn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Este modelo no es apropiado para esta tarea, es peor que lanzar una moneda al aire, tiene el accuracy m√°s bajo de todos los modelos solo 0.54. Como podemos ver claramente en la matriz de confusi√≥n, esta clasificando casi todo como IA. Eso explica que el recall de IA sea muy alto (0.97) mientras que el de la clase Humano es 0.06.\n",
        "\n",
        "Este comportamiento es t√≠pico cuando solo se entrena la capa de clasificaci√≥n: BERT aporta representaciones ricas, pero como no dejamos cambiar los pesos del modelo las representaciones siguen siendo demasiado gen√©ricas. El modelo base de BERT sin ajuste no considera el estilo instructivo como relevante y la clasificaci√≥n no tiene poder suficiente para encontrar el patr√≥n por lo que colapsa y aprende la regla m√°s f√°cil: \"en caso de duda es IA\".\n",
        "\n",
        "A√∫n as√≠, este experimento es √∫til para demostrar que congelar todo apenas adapta el modelo al corpus."
      ],
      "metadata": {
        "id": "uhhqU2B5gXCv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###5.2.2. Experimento 1.2: Entrenar la √∫ltima capa de BERT"
      ],
      "metadata": {
        "id": "KLWHmelGVl1s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\\033[1mExperimento 1.2: Fine-tuning ligero (descongelo √∫ltima capa)\\033[0m\")\n",
        "\n",
        "# Cargar modelo base\n",
        "ModelLight_2 = AutoModelForSequenceClassification.from_pretrained(\n",
        "    ModelName,\n",
        "    num_labels=2\n",
        ")\n",
        "# Congelar todo el modelo base\n",
        "for p in ModelLight_2.base_model.parameters():\n",
        "    p.requires_grad = False\n",
        "\n",
        "# Descongelar √∫ltima capa\n",
        "for p in ModelLight_2.base_model.encoder.layer[-1].parameters():\n",
        "    p.requires_grad = True\n",
        "\n",
        "# Argumentos de entrenamiento\n",
        "TrainingArgsLight_2 = TrainingArguments(\n",
        "    output_dir=\"./ResultsLight_2\",\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"f1\",\n",
        "    report_to=\"none\",\n",
        "    logging_dir=\"./logs_light_2\",\n",
        "    logging_steps=100,\n",
        ")\n",
        "\n",
        "# Trainer\n",
        "trainer_light_2 = Trainer(\n",
        "    model=ModelLight_2,\n",
        "    args=TrainingArgsLight_2,\n",
        "    train_dataset=tokenizedTrain,\n",
        "    eval_dataset=tokenizedTest,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "# Entrenar modelo y medir tiempo\n",
        "start = time.time()\n",
        "trainer_light_2.train()\n",
        "TimeLight_2 = time.time() - start\n",
        "TimeLight_2_min = TimeLight_2 / 60\n",
        "\n",
        "print(f\"Tiempo de entrenamiento: {TimeLight_2:.2f} s ({TimeLight_2_min:.2f} min)\")"
      ],
      "metadata": {
        "id": "RjNTRV77VwVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###5.2.2.1. Evaluaci√≥n y an√°lisis Experimento 1.2"
      ],
      "metadata": {
        "id": "hmeAhEGhhHRn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\\033[1mEvaluaci√≥n Experimento 1.2 (BERT √∫ltima capa entrenable)\\033[0m\")\n",
        "ResultsLight_2 = trainer_light_2.evaluate()\n",
        "print(ResultsLight_2)\n",
        "\n",
        "PredsLight_2 = trainer_light_2.predict(tokenizedTest)\n",
        "y_true_2 = PredsLight_2.label_ids\n",
        "YPred_2 = PredsLight_2.predictions.argmax(axis=1)\n",
        "\n",
        "print(\"\\n\\033[1mInforme de clasificaci√≥n - Experimento 1.2 (BERT √∫ltima capa entrenable)\\033[0m\")\n",
        "print(classification_report(y_true_2, YPred_2, target_names=LabelNames))\n",
        "\n",
        "cm_2 = confusion_matrix(y_true_2, YPred_2)\n",
        "plt.figure(figsize=(4.5, 4))\n",
        "sns.heatmap(cm_2, annot=True, fmt=\"d\", cmap=\"GnBu\",\n",
        "            xticklabels=LabelNames, yticklabels=LabelNames)\n",
        "plt.title(\"Matriz de confusi√≥n ‚Äì Experimento 1.2 (BERT √∫ltima capa entrenable)\")\n",
        "plt.xlabel(\"Clase predicha\")\n",
        "plt.ylabel(\"Clase real\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ijhCaJkbWz5A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Con solo permitir que la √∫ltima capa se adapte, BERT deja de predecir todos los documentos como IA y separa bien las dos clases. El error se reparte de forma razonable: algunos texto humanos los confunde como IA generativa y viceversa. El rango de rendimiento mejora significativamente respecto al experimento anterior e incluso supera el segundo modelo LSTM, con un accuracy de 0.88.\n",
        "\n",
        "Este resultado es muy importante porque muestra que el fine-tuning no es sin√≥nimo de congelar todo. Esto es un ejemplo perfecto de la eficiencia del Transfer Learning: no necesitamos re-entrenar 170M de par√°metros, solo los √∫ltimos millones, para adaptar un modelo gen√©rico a un problema espec√≠fico.Queda claro que descongelando las √∫ltimas capas la mejora es enorme sin tener que aumentar apenas el tiempo, este modelo necesita aprox. 8 minutos para entrenar."
      ],
      "metadata": {
        "id": "CrkIIt4whqEY"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "clzuWNr9AuAr"
      },
      "source": [
        "##5.3. Experimento 2: Fine-tuning Completo\n",
        "\n",
        "En este apartado no se congela ninguna capa, se permite que todas las capas del Transformer se adapten al patr√≥n concreto de este corpus. De esta manera el modelo puede ajustar todos los pesos del modelo, desde la capa de embedding hasta la clasificaci√≥n.\n",
        "\n",
        "Este experimento es computacionalmente mucho m√°s costos pero a cambio, permite al modelo especializar su conocimiento del lenguaje a nuestro Dataset. Este m√©todo es la forma m√°s estandar de fine-tuning.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0K2C1exbAwbY"
      },
      "outputs": [],
      "source": [
        "print(\"\\n\\033[1mExperimento 2: Fine-tuning completo\\033[0m\")\n",
        "\n",
        "# Cargar el modelo sin congelar\n",
        "ModelFull = AutoModelForSequenceClassification.from_pretrained(\n",
        "    ModelName,\n",
        "    num_labels=2\n",
        ")\n",
        "\n",
        "# Configurar entrenamiento\n",
        "TrainingArgsFull = TrainingArguments(\n",
        "    output_dir=\"./ResultsFull\",\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    learning_rate=2e-5,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"f1\",\n",
        "    report_to=\"none\",\n",
        "    logging_dir=\"./logs_full\",\n",
        "    logging_steps=100,\n",
        "    warmup_ratio=0.1,\n",
        ")\n",
        "\n",
        "# Trainer\n",
        "TrainerFull = Trainer(\n",
        "    model=ModelFull,\n",
        "    args=TrainingArgsFull,\n",
        "    train_dataset=tokenizedTrain,\n",
        "    eval_dataset=tokenizedTest,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "# Entrenar modelo y medir tiempo\n",
        "start = time.time()\n",
        "TrainerFull.train()\n",
        "TimeFull = time.time() - start\n",
        "TimeFull_min = TimeFull / 60\n",
        "\n",
        "print(f\"Tiempo de entrenamiento: {TimeFull:.2f} s ({TimeFull_min:.2f} min)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###5.3.1. Evaluaci√≥n y an√°lisis Experimento 2"
      ],
      "metadata": {
        "id": "QlWva5XXjEsr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\\033[1mEvaluaci√≥n (fine-tuning completo)\\033[0m\")\n",
        "ResultsFull = TrainerFull.evaluate()\n",
        "print(ResultsFull)\n",
        "\n",
        "\n",
        "PredsFull = TrainerFull.predict(tokenizedTest)\n",
        "y_true_full = PredsFull.label_ids\n",
        "YPred_full = PredsFull.predictions.argmax(axis=1)\n",
        "\n",
        "print(\"\\n\\033[1mInforme de clasificaci√≥n (BERT fine-tuning completo)\\033[0m\")\n",
        "print(classification_report(y_true_full, YPred_full, target_names=LabelNames))\n",
        "\n",
        "cm_full = confusion_matrix(y_true_full, YPred_full)\n",
        "plt.figure(figsize=(4.5, 4))\n",
        "sns.heatmap(cm_full, annot=True, fmt=\"d\", cmap=\"GnBu\",\n",
        "            xticklabels=LabelNames, yticklabels=LabelNames)\n",
        "plt.title(\"Matriz de confusi√≥n ‚Äì BERT (fine-tuning completo)\")\n",
        "plt.xlabel(\"Clase predicha\")\n",
        "plt.ylabel(\"Clase real\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "m0BT9FL3W1_B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Este es el mejor modelo de los 3 sube aprox 3 puntos de accuracy respecto a Experimento 1.2 de 0.89 a 0.92. Adem√°s, reduce los dos errores. El modelo esta completamente especializado al dataset y puede encontrar patrones estil√≠sticos de la IA.\n",
        "\n",
        "Esto indica que dejando entrenando todo el encoder, BERT es capaz de especializar las cabezas de atenci√≥n en las estructura que vimos en el an√°lisis del apartado 1 y no solo en el l√©xico. Aunque el coste es claro y hay que evaluar si la mejora merece la pena porque este modelo necesita m√°s de 18 minutos de entrenamiento fren a los 8 de los modelos ligeros."
      ],
      "metadata": {
        "id": "gprIy3I-jIv0"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kl707fmCA0Ia"
      },
      "source": [
        "##5.4. Comparativa y Justificaci√≥n\n",
        "\n",
        "La comparaci√≥n de los 3 experimentos de fine-tuning revela un claro intercambio entre coste-beneficio.\n",
        "\n",
        "Al saltar del 1.1 (BERT totalmente congelado) al 1.2 (descongelar la √∫ltima capa de BERT) obtuvimos aproximadamente 0.47 m√°s de accuracy a cambio de un aumento de coste computacional casi nulo (solo el 4% del tiempo). Este avance es el m√°s eficiente y rentable.\n",
        "\n",
        "Al saltar del 1.2 (descongelar la √∫ltima capa de BERT) al 2 (fine-tuning completo) obtuvimos 3 puntos m√°s de accuracy a cambio de m√°s del doble de tiempo de entrenamiento de 8 minutos a casi 20 minutos, porque ahora se actualizan los ~170M de par√°metros del modelo. Aqu√≠ la mejora es marginal respecto al sobrecoste.\n",
        "\n",
        "En cuanto a la relaci√≥n entre el tama√±o del modelo y el de los datos, hay que tener en cuenta que el modelo es muy grande comparado con el corpus. Entrenar un modelo tan grande desde cero hubiera resultado en sobreajuste. El resultado se debe al transfer learning que usa los documentos para especializar un modelo que ya ha aprendido el idioma gracias a su preentrenamiento.\n",
        "\n",
        "Teniendo esto en cuenta podemos concluir que para la mayor√≠a de aplicaciones pr√°cticas utilizar fine-tuning ligero de las √∫ltimas capas es apostar por el ganador. Ofrece un gran rendimiento con un coste computacional razonable.\n",
        "\n",
        "El segundo modelo solo se justifica si esos 3 puntos porcentuales fueran absolutamente cr√≠ticos y se estuviera dispuesta a pagar el precio.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Xu6TMb_w-dw"
      },
      "source": [
        "# **6. Requisito Extra: An√°lisis de atenciones internas**\n",
        "\n",
        "En el apartado 5, se demostro que el fine-tuning completo de un modelo Transformer tiene el mejor rendimiento entre todos los modelos evaluados a lo largo de la pr√°ctica.\n",
        "\n",
        "El motivo de esta superioridad se debe a la arquitectura del Transformer¬∫ y el mecanismo de auto-atenci√≥n. Este mecanismo permite al Transformer ponderar todas las palabras de la secuencia al mismo tiempo y no de forma secuencial como en la LSTM o de manera aislada como en los modelo cl√°sicos.\n",
        "\n",
        "En este apartado, se va un paso m√°s all√°, no se mide el rendimiento sino que, se inspecciona el comportamiento interno. Para ello, se utiliza la librer√≠a `bertviz` con el objetivo de visualizar a qu√© tokens presta atenci√≥n el modelo cuando decide si un texto es Humano o IA. Esto permite comprobar si el modelo aprende las caracter√≠sticas de estilo (conectores, tono instructivo) que se detectaron en el apartado 1 o si est√° usando otros indicios.\n",
        "\n",
        "Con esta evaluaci√≥n el objetivo es responder a las preguntas: ¬øPor qu√© funciona tan bien el modelo? y ¬øA qu√© est√° prestando atenci√≥n para tomar la decisi√≥n?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NktthwO30EwE"
      },
      "source": [
        "##6.1. Configuraci√≥n del modelo\n",
        "\n",
        "En la secci√≥n anterior, para usar el modelo de Hugging Face bastaba con cargarlo. Para poder hacer el an√°lisis de atenci√≥n, primero, hay que carga el checkpoint afinado (guardado por el Trainer), y tambi√©n, especificar al modelo explic√≠tamente que devuelva los pesos de atenci√≥n (output_attentions=True). Adem√°s, se debe usar el mismo tokenizador con el que se entren√≥.\n",
        "\n",
        "El m√©todo de an√°lisis m√°s efectivo es analizar el token [CLS], ya que es el que resumen toda la secuencia y es el que se utiliza para la clasificaci√≥n.\n",
        "\n",
        "Por lo tanto, visualizar a qu√© palabras atiende [CLS] en las √∫ltimas capas (capas de clasificaci√≥n) se puede deducir que se√±ales utiliza el modelo para decidir la clase."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.1.1. Selecci√≥n √∫ltimo checkpoint del fine-tuning\n",
        "\n",
        "Este bloque busca en `./ResultsFull` el checkpoint m√°s reciente. Ese checkpoint se usa en `ModelPath` para cargar el modelo ya afinado y poder analizar sus atenciones en lugar del modelo base."
      ],
      "metadata": {
        "id": "NvlQix8f86Cz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_dir = \"./ResultsFull\"\n",
        "checkpoints = [d for d in os.listdir(base_dir) if d.startswith(\"checkpoint-\")]\n",
        "checkpoints = sorted(checkpoints, key=lambda x: int(x.split(\"-\")[1]))\n",
        "print(checkpoints[-1])"
      ],
      "metadata": {
        "id": "Mp-fS9Y9e670"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.1.2. Carga del modelo afinado\n",
        "\n",
        "En este bloque se carga el modelo, activando `output_attentions=True` para que devuelva los mapas de atenci√≥n.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "wlE6E2669xId"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "olib1wSd2G3S"
      },
      "outputs": [],
      "source": [
        "# Si se va a volver a entrenar, hay que mirar el resultado del apartado anterior y cambiar \"1356\" por el checkpoint correcto\n",
        "ModelPath = \"./ResultsFull/checkpoint-1356\"\n",
        "\n",
        "# Tokenizador del mismo modelo\n",
        "tokenizer = AutoTokenizer.from_pretrained(ModelName)\n",
        "\n",
        "# Intentar cargar el modelo fine-tuneado y pedir atenciones\n",
        "try:\n",
        "    model_viz = AutoModelForSequenceClassification.from_pretrained(\n",
        "        ModelPath,\n",
        "        output_attentions=True\n",
        "    )\n",
        "    model_viz.eval()\n",
        "except Exception:\n",
        "    model_viz = AutoModelForSequenceClassification.from_pretrained(\n",
        "        ModelName,\n",
        "        output_attentions=True,\n",
        "        num_labels=2\n",
        "    )\n",
        "    model_viz.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.1.3. Funci√≥n auxiliar de visualizaci√≥n\n",
        "\n",
        "Para analizar las atenciones del modelo se ha implementado una funci√≥n auxiliar `ShowAttention()` junto con varias funciones de apoyo. La idea es tener un resumen compacto y legible de lo que est√° mirando el Transformer, en lugar de quedarnos solo con los mapas de atenci√≥n brutos.\n",
        "\n",
        "La l√≥gica general es:\n",
        "\n",
        "* **Patrones de estilo del EDA**\n",
        "\n",
        "  * Se definen dos conjuntos, `ConnNgramas` e `IATokens`, con conectores y muletillas t√≠picas de textos IA detectados en el EDA\n",
        "  * No cambian la predicci√≥n\n",
        "  * sirven para etiquetar r√°pido si el texto usa ‚Äúplantillas‚Äù de estilo IA\n",
        "\n",
        "* **Reconstrucci√≥n de palabras completas**\n",
        "\n",
        "  * `FusionarSubpalabras` junta subpalabras WordPiece (tokens que empiezan por `##`) y suma sus puntuaciones, para trabajar con palabras completas en vez de trocitos\n",
        "\n",
        "* **Selecci√≥n de tokens m√°s relevantes**\n",
        "\n",
        "  * `TopTokensAtencion` filtra los tokens especiales y devuelve las *k* palabras con mayor score de atenci√≥n\n",
        "\n",
        "* **Detecci√≥n de se√±ales r√°pidas de IA**\n",
        "\n",
        "  * `DetectSenalesIA` comprueba si el texto tiene:\n",
        "\n",
        "    * Listas numeradas tipo receta\n",
        "    * N-gramas ‚Äúplantilla‚Äù de gu√≠a\n",
        "    * Muletillas frecuentes de la clase IA\n",
        "\n",
        "  Esto se resume en una peque√±a tabla\n",
        "\n",
        "* **Funci√≥n principal `ShowAttention`**\n",
        "\n",
        "  * Pide al modelo las atenciones\n",
        "  * Combina las capas dando m√°s de peso a las √∫ltimas\n",
        "  * Calcula un score medio por token\n",
        "  * Reconstruye palabras completas\n",
        "  * Calcula los *Top-K* tokens m√°s atendidos\n",
        "  * Muestra:\n",
        "\n",
        "    * Resumen estructurado\n",
        "    * Tabla con los tokens m√°s relevantes seg√∫n la atenci√≥n\n",
        "    * Widget interactivo de `bertviz` para inspeccionar las cabezas de atenci√≥n en detalle"
      ],
      "metadata": {
        "id": "PRXUYgIl9-91"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# N-gramas t√≠picos de textos IA encontrados en el EDA\n",
        "ConnNgramas = {\n",
        "    # estructura de gu√≠a\n",
        "    \"primer lugar\", \"primer paso\", \"primero debes\", \"debes hacer\", \"c√≥mo hacerlo\",\n",
        "    \"paso paso\", \"explicaremos c√≥mo\", \"aqu√≠ explicamos c√≥mo\",\n",
        "    \"continuaci√≥n presentamos\", \"presentamos consejos\",\n",
        "\n",
        "    # conectores y muletillas discursivas\n",
        "    \"importante tener cuenta\", \"debes tener cuenta\", \"tener cuenta\",\n",
        "    \"puede ser tarea\", \"puede parecer tarea\",\n",
        "\n",
        "    # trigramas\n",
        "    \"parecer tarea desalentadora\", \"tarea desalentadora\",\n",
        "    \"tarea complicada\", \"tarea dif√≠cil\", \"tarea sencilla\",\n",
        "    \"sigue leyendo\", \"sigue leyendo descubrir\",\n",
        "    \"excelente opci√≥n\", \"manera efectiva\",\n",
        "    \"importante saber c√≥mo\",\n",
        "}\n",
        "\n",
        "# Muletillas que en el EDA aparec√≠an mucho en textos IA\n",
        "IATokens = {\n",
        "    \"importante\", \"primero\", \"primer\", \"debes\", \"consejos\",\n",
        "    \"continuaci√≥n\", \"presentamos\", \"explicaremos\", \"paso\", \"pasos\",\n",
        "    \"proceso\", \"embargo\", \"adem√°s\", \"cuenta\", \"diferentes\", \"opci√≥n\",\n",
        "}\n",
        "\n",
        "# Junta subpalabras WordPiece en una sola palabra y suma sus puntuaciones\n",
        "def FusionarSubpalabras(tokens, scores):\n",
        "\n",
        "    OutTokens, OutScores = [], []\n",
        "    BufToken, BufScore = \"\", 0.0\n",
        "\n",
        "    def volcar_buffer():\n",
        "        nonlocal BufToken, BufScore\n",
        "        if BufToken:\n",
        "            OutTokens.append(BufToken)\n",
        "            OutScores.append(BufScore)\n",
        "            BufToken, BufScore = \"\", 0.0\n",
        "\n",
        "    for t, s in zip(tokens, map(float, scores)):\n",
        "        if t in (\"[CLS]\", \"[SEP]\", \"[PAD]\"):\n",
        "            volcar_buffer()\n",
        "            OutTokens.append(t)\n",
        "            OutScores.append(s)\n",
        "            continue\n",
        "\n",
        "        if t.startswith(\"##\"):\n",
        "            # continuaci√≥n de la palabra anterior\n",
        "            BufToken += t[2:]\n",
        "            BufScore += s\n",
        "        else:\n",
        "            volcar_buffer()\n",
        "            BufToken, BufScore = t, s\n",
        "\n",
        "    volcar_buffer()\n",
        "    return OutTokens, OutScores\n",
        "\n",
        "# Devuelve las k palabras con mayor score de atenci√≥n\n",
        "def TopTokensAtencion(tokens, scores, k=10):\n",
        "\n",
        "    pares = [\n",
        "        (t, float(s))\n",
        "        for t, s in zip(tokens, scores)\n",
        "        if t not in (\"[CLS]\", \"[SEP]\", \"[PAD]\") and re.search(r\"\\w\", t)\n",
        "    ]\n",
        "    pares.sort(key=lambda x: x[1], reverse=True)\n",
        "    return pares[:k]\n",
        "\n",
        "# Busca se√±ales que, tras el an√°lisis del corpus, son t√≠picas de IA\n",
        "def DetectSenalesIA(tokens):\n",
        "\n",
        "    txt = \" \".join(tokens).lower()\n",
        "\n",
        "    # Listas numeradas\n",
        "    numerada = bool(re.search(r\"(^|\\s)(\\d+[\\.\\)\\-])\\s\", txt))\n",
        "\n",
        "    # Conectores\n",
        "    ConectoresNg = [c for c in ConnNgramas if c in txt]\n",
        "\n",
        "    # Muletillas del conjunto IATokens\n",
        "    toks = [t.lower() for t in tokens if t not in (\"[CLS]\", \"[SEP]\", \"[PAD]\")]\n",
        "    ConectoresTok = sorted({t for t in toks if t in IATokens})\n",
        "\n",
        "    return numerada, ConectoresNg, ConectoresTok\n",
        "\n",
        "\n",
        "def ShowAttention(\n",
        "    text,\n",
        "    tokenizer,\n",
        "    model,\n",
        "    *,\n",
        "    title=\"An√°lisis de atenci√≥n\",\n",
        "    top_k=10,\n",
        "    max_length=256,\n",
        "    TextMaxCar=500,\n",
        "):\n",
        "\n",
        "    print(\"\\n\" + title)\n",
        "    print(text, \"\\n\")\n",
        "\n",
        "    # Pedir atenciones al modelo\n",
        "    model.eval()\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=max_length)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs, output_attentions=True)\n",
        "\n",
        "    atts = outputs.attentions\n",
        "\n",
        "    # Tensor de atenciones\n",
        "    A = torch.stack(atts, dim=0)\n",
        "    if A.ndim == 5:\n",
        "        A = A.squeeze(1)\n",
        "    elif A.ndim != 4:\n",
        "        raise ValueError(f\"Forma inesperada de attentions: {A.shape}\")\n",
        "\n",
        "    # capas, cabezas, tokens\n",
        "    L, H, T, _ = A.shape\n",
        "\n",
        "    # Dar m√°s peso a las √∫ltimas capas\n",
        "    w = torch.ones(L)\n",
        "    if L >= 4:\n",
        "        w[-4:] = 1.5\n",
        "    w = (w / w.sum()).view(L, 1, 1, 1)\n",
        "    A = A * w\n",
        "\n",
        "    # Score medio de atenci√≥n\n",
        "    cls_idx = 0\n",
        "    score = (\n",
        "        A[:, :, cls_idx, :].mean((0, 1)) +\n",
        "        A[:, :, :, cls_idx].mean((0, 1))\n",
        "    ) / 2.0\n",
        "\n",
        "    # Reconstruir palabras completas\n",
        "    wordpieces = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
        "    tokens_m, scores_m = FusionarSubpalabras(wordpieces, score.tolist())\n",
        "\n",
        "    # Top-K tokens m√°s atendidos\n",
        "    top_tokens = TopTokensAtencion(tokens_m, scores_m, k=top_k)\n",
        "\n",
        "    # Tabla de se√±ales de estilo\n",
        "    numerada, ConectoresNg, ConectoresTok = DetectSenalesIA(tokens_m)\n",
        "    resumen = {\n",
        "        \"Enumeraci√≥n\": \"S√≠\" if numerada else \"No\",\n",
        "        \"Conectores n-grama\": \", \".join(ConectoresNg) if ConectoresNg else \"‚Äî\",\n",
        "        \"Muletillas IA\": \", \".join(ConectoresTok) if ConectoresTok else \"‚Äî\",\n",
        "    }\n",
        "    display(pd.DataFrame([resumen]))\n",
        "\n",
        "    # Tabla con los tokens m√°s atendidos\n",
        "    df_top = pd.DataFrame(top_tokens, columns=[\"Token\", \"Score (CLS‚Üîtoken)\"])\n",
        "    display(df_top)\n",
        "\n",
        "    # Visualizaci√≥n interactiva de cabezas de atenci√≥n\n",
        "    display(head_view(outputs.attentions, wordpieces))\n",
        "\n",
        "    return {\n",
        "        \"top_tokens\": top_tokens,\n",
        "        \"numeracion_tipo_lista\": numerada,\n",
        "        \"ConectoresNgram\": ConectoresNg,\n",
        "        \"ConectoresTokens\": ConectoresTok,\n",
        "        \"longitud_tokens\": len(tokens_m),\n",
        "    }\n"
      ],
      "metadata": {
        "id": "TPCXfWUr99ix"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###6.1.4. Selecci√≥n de ejemplos representativos\n",
        "\n",
        "Antes de analizar casos concretos, es necesario obtener ejemplos representativos del conjunto de test.\n",
        "\n",
        "Al principio intente elegir los textos al azar, pero no consegu√≠a ejemplos para errores de clasificaci√≥n. Por eso, decid√≠ seleccionar ejemplos de las 4 situaci√≥nes posibles en una matriz de confusi√≥n:\n",
        "  * **Verdadero positivo**: acierto en textos generados por IA\n",
        "  * **Verdadero negativo**: acierto en textos humanos\n",
        "  * **Falso negativo:** para el modelo la IA parece humana\n",
        "  * **Falso positivo:** el humano parece IA para el modelo\n",
        "\n",
        "Para construir estos grupos, se recorre el `DatasetTest` y se predice la clase de cada documento con el modelo entrenado en el Experimento 2 del apartado 5. Se guardan hasta *MaxTipo* documentos por categor√≠a y, posteriormente, se ordenan por longitud de texto para poder elegir ejemplos representativos pero relativamente cortos, que resultan m√°s manejables a la hora de inspeccionar manualmente las atenciones."
      ],
      "metadata": {
        "id": "VVTKxTPdchIP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MaxTipo = 5\n",
        "\n",
        "IAaIA = []    # real=1, pred=1\n",
        "HumaHum = []  # real=0, pred=0\n",
        "IAaHum = []   # real=1, pred=0\n",
        "HumaIA = []   # real=0, pred=1\n",
        "\n",
        "for i in range(len(DatasetTest)):\n",
        "    row = DatasetTest[i]\n",
        "    texto = row[\"contenido\"]\n",
        "    y_real = int(row[\"label\"])\n",
        "\n",
        "    # Tokenizar\n",
        "    inputs = tokenizer(\n",
        "        texto,\n",
        "        return_tensors=\"pt\",\n",
        "        truncation=True,\n",
        "        max_length=256,\n",
        "        padding=\"max_length\",\n",
        "    )\n",
        "\n",
        "    # Inferir con el modelo ya cargado\n",
        "    with torch.no_grad():\n",
        "        outputs = model_viz(**inputs)\n",
        "        logits = outputs.logits\n",
        "        YPred = logits.argmax(dim=1).item()\n",
        "\n",
        "    # Clasificar documento\n",
        "    if y_real == 1 and YPred == 1:\n",
        "        if len(IAaIA) < MaxTipo:\n",
        "            IAaIA.append((i, texto, y_real, YPred))\n",
        "    elif y_real == 0 and YPred == 0:\n",
        "        if len(HumaHum) < MaxTipo:\n",
        "            HumaHum.append((i, texto, y_real, YPred))\n",
        "    elif y_real == 1 and YPred == 0:\n",
        "        if len(IAaHum) < MaxTipo:\n",
        "            IAaHum.append((i, texto, y_real, YPred))\n",
        "    elif y_real == 0 and YPred == 1:\n",
        "        if len(HumaIA) < MaxTipo:\n",
        "            HumaIA.append((i, texto, y_real, YPred))\n",
        "\n",
        "    # Terminar bucle si tiene todos los ejemplos\n",
        "    if (\n",
        "        len(IAaIA) == MaxTipo\n",
        "        and len(HumaHum) == MaxTipo\n",
        "        and len(IAaHum) == MaxTipo\n",
        "        and len(HumaIA) == MaxTipo\n",
        "    ):\n",
        "        break\n",
        "\n",
        "# Ordenar cada lista por longitud de texto (corto a largo)\n",
        "IAaIA   = sorted(IAaIA,   key=lambda x: len(x[1]), reverse=False)\n",
        "HumaHum = sorted(HumaHum, key=lambda x: len(x[1]), reverse=False)\n",
        "IAaHum  = sorted(IAaHum,  key=lambda x: len(x[1]), reverse=False)\n",
        "HumaIA  = sorted(HumaIA,  key=lambda x: len(x[1]), reverse=False)\n",
        "\n",
        "def ImprimirLista(titulo, lista):\n",
        "    print(f\"\\n\\033[1m{titulo} (total {len(lista)})\\033[0m\")\n",
        "    if not lista:\n",
        "        print(\"No se encontraron ejemplos.\")\n",
        "    else:\n",
        "        for idx, (i, txt, real, pred) in enumerate(lista, start=1):\n",
        "            print(f\"\\n  Ejemplo: {idx}\")\n",
        "            print(f\"    idx: {i}\")\n",
        "            print(f\"    Real: {real} ({'IA' if real==1 else 'Humano'})\")\n",
        "            print(f\"    Pred : {pred} ({'IA' if pred==1 else 'Humano'})\")\n",
        "            print(f\"    Longitud: {len(txt)} caracteres\")\n",
        "            print(f\"    {txt[:280]}...\\n\")\n",
        "\n",
        "# Imprimir cada lista\n",
        "ImprimirLista(\"IA ‚Üí IA (acierto)\", IAaIA)\n",
        "ImprimirLista(\"Humano ‚Üí Humano (acierto)\", HumaHum)\n",
        "ImprimirLista(\"IA ‚Üí Humano (falso negativo)\", IAaHum)\n",
        "ImprimirLista(\"Humano ‚Üí IA (falso positivo)\", HumaIA)\n"
      ],
      "metadata": {
        "id": "ekn4aBP7clJq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##6.2. Caso 1: texto de IA clasificado correctamente\n",
        "\n",
        "En este primer caso se analiza un texto generado por IA que el modelo clasifica correctamente como IA (verdadero positivo). El objetivo es comprobar si el modelo se apoya en las se√±ales de estilo que se detectaron en el EDA: conectores largos, tono instructivo, estructuras tipo ‚ÄúEn primer lugar‚Ä¶‚Äù, ‚ÄúA continuaci√≥n‚Ä¶‚Äù, etc."
      ],
      "metadata": {
        "id": "IXhvTFKgcAtk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "texto = DatasetTest[2][\"contenido\"]\n",
        "_ = ShowAttention(\n",
        "    texto, tokenizer, model_viz,\n",
        "    top_k=10,\n",
        "    title=\"Texto IA clasificado correctamente\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "UjUfyTqYb_vw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_EepIP4l2JsS"
      },
      "source": [
        "El modelo reconoce el patr√≥n que ya se observo en el primer apartado (EDA) es por eso que es capaz de clasificar el texto generado con IA correctamente. El modelo acierta porque capta tanto el l√©xico t√©cnico (\"infecciones\", \"correctamente\", \"excesiva\"), como los conectores organizadores (\"sin embargo\", \"por eso\"), como muletillas detectadas en los N-gramas (\"importante\" o \"primero\"). La atenci√≥n agregada se concentra en conectores lo que encaja con la hip√≥tesis planteada, la IA usa un tono instructivo y un estilo estructurado."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##6.3. Caso 2: texto humano clasificado correctamente\n",
        "\n",
        "En este segundo caso se analiza un texto humano que el modelo clasifica correctamente como Humano (verdadero negativo). Aqu√≠ interesa ver si el modelo acierta por presencia de alg√∫n patr√≥n espec√≠fico, o m√°s bien por ausencia de las plantillas t√≠picas de IA."
      ],
      "metadata": {
        "id": "w6-qSHDqcGLe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "texto = DatasetTest[3][\"contenido\"]\n",
        "_ = ShowAttention(\n",
        "    texto, tokenizer, model_viz,\n",
        "    top_k=10,\n",
        "    title=\"Texto humano clasificado correctamente\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "vZlL5jWCcIHq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "El modelo es capaz de reproducir el patr√≥n observado en el EDA y por eso, clasifica el texto humano correctamente. Acierta porque detecta ausencia de conectores orginazadores t√≠picos de IA, mientras que encuentra l√©xico descriptivo como \"postura\", \"t√≠mida\", o \"nerviosa\" junto con un verbo imperativo: \"Observa\". La atenci√≥n agregada se concentra en sustantivos y adjetivos que no son muletillas, esto encaja con la hip√≥tesis inicial, los textos humanos se apoyan en observaciones y no tanto en un tono instructivo. El modelo ha aprendido que los textos Humanos tiene un estilo menos normativo y m√°s coloquial"
      ],
      "metadata": {
        "id": "g615V6vscJdo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##6.4. Caso 3: Falso negativo (IA ‚Üí Humano)\n",
        "\n",
        "En este tercer caso se analiza un error del modelo: un texto generado por IA que el modelo clasifica err√≥neamente como Humano (falso negativo). Este tipo de an√°lisis es clave para entender las limitaciones reales del modelo."
      ],
      "metadata": {
        "id": "UyVlVNUocLpU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "texto = DatasetTest[124][\"contenido\"]\n",
        "_ = ShowAttention(\n",
        "    texto, tokenizer, model_viz,\n",
        "    top_k=10,\n",
        "    title=\"Texto IA clasificado como humano (falso negativo)\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "a8J6B0hKcNoZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "En este caso el modelo falla la predicci√≥n porque, aunque el texto sea IA, no usa los conectores discursivos que el clasificador aprendi√≥ como se√±a de identidad.\n",
        "\n",
        "En su lugar, aparecen imperativos (‚Äúllene‚Äù, ‚Äúlleve‚Äù, ‚Äúponga‚Äù) y objetos concretos (‚Äúagua tibia‚Äù, ‚Äújab√≥n suave‚Äù). La atenci√≥n agregada lo respalda: se concentra en verbos de acci√≥n y sustantivos, sin rastro de conectores ni muletillas t√≠picas de IA.\n",
        "\n",
        "En resumen, el clasificador depende de esos conectores para identificar IA y, adem√°s, asocia los imperativos con textos humanos, lo que empuja este caso a un falso negativo."
      ],
      "metadata": {
        "id": "UfZN6wfncQAn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##6.5. Caso 4: Falso positivo (Humano ‚Üí IA)\n",
        "\n",
        "Por √∫ltimo se analiza el caso complementario: un texto humano que el modelo clasifica err√≥neamente como IA (falso positivo). Aqu√≠ interesa ver si el autor humano, de forma natural, ha utilizado un estilo muy parecido al de la IA (por ejemplo, demasiados conectores, tono excesivamente instructivo o estructura muy ‚Äúlimpia‚Äù)."
      ],
      "metadata": {
        "id": "uG2ON1rh6rTW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "texto = DatasetTest[24][\"contenido\"]\n",
        "_ = ShowAttention(\n",
        "    texto, tokenizer, model_viz,\n",
        "    top_k=10,\n",
        "    title=\"Texto humano clasificado como IA (falso positivo)\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "ZoxVWXVQ7FoK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "En este caso el modelo comete un falso positivo: aunque el texto es humano, presenta una redacci√≥n t√©cnica. La atenci√≥n agregada se concentra en t√©rminos de plataforma y dominio ‚Äî‚Äúcoaxial‚Äù, ‚Äúinform√°tica‚Äù, ‚Äúacceso‚Äù‚Äî que, en este dataset, aparecen con frecuencia en textos generados, de modo que el clasificador ha aprendido a asociarlos con IA.\n",
        "\n",
        "Aun sin conectores ni enumeraciones, el tono instruccional/operativo (‚Äúhacer X, luego Y‚Ä¶‚Äù) basta para activar la etiqueta de IA. En resumem, este falso positivo muestra que el modelo relaciona la escritura ‚Äúautomatizada‚Äù con IA, incluso cuando no aparecen los conectores can√≥nicos."
      ],
      "metadata": {
        "id": "9otNaLiL76Mg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "La visualizaci√≥n de atenciones refuerza la conclusi√≥n del EDA: lo que realmente distingue a la clase IA no es el tema, sino el estilo.\n",
        "\n",
        "El Transformer internaliza ese patr√≥n y dirige su atenci√≥n a conectores y estructuras discursivas (‚Äúsin embargo‚Äù, ‚Äúpor eso‚Äù, ‚Äúlo primero‚Äù‚Ä¶), lo que explica que supere al resto de modelos, m√°s sensibles al l√©xico y menos a la organizaci√≥n del discurso.\n",
        "\n",
        "El an√°lisis tambi√©n hace visible los sesgos del clasificador: cuando un texto de IA no usa conectores o cuando un texto humano adopta tono instruccional, la atenci√≥n pierde su pista principal y aparecen falsos positivos/negativos. En otras palabras, como el modelo generaliza por estilo, si este se diluye, su fiabilidad cae.\n",
        "\n",
        "En conjunto con el apartado 5, este apartado justifica el buen rendimiento del Transformer al explotar se√±ales de estilo y acota su alcance: es pr√°cticamente infalible cuando hay conectores o estructura expl√≠cita, y fr√°gil cuando domina un estilo imperativo sin muletillas."
      ],
      "metadata": {
        "id": "tKgCEZxLcSwD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **7. Discusi√≥n integradora**\n",
        "\n",
        "Este apartado final tiene como objetivo integrar los resultados obtenidos en todas las fases de la pr√°ctica, desde el EDA inicial hasta los modelos Transformer con fine-tuning completo.\n",
        "\n",
        "Se analizan: qu√© t√©cnicas han funcionado mejor y en qu√© condiciones, qu√© limitaciones presentan los modelos cl√°sicos, cu√°les son los principales trade-offs entre interpretabilidad, coste computacional y rendimiento, y qu√© implicaciones tiene todo ello para problemas reales de detecci√≥n de texto generado por IA."
      ],
      "metadata": {
        "id": "JOiWNTeD10_u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### 7.1. ¬øQu√© t√©cnicas funcionaron mejor y en qu√© condiciones?\n",
        "\n",
        "Los resultados confirman que el rendimiento de cada familia de modelos est√° condicionado por su capacidad para capturar contexto y estilo, que el Apartado 1 identific√≥ como la se√±al clave para este problema: textos IA con un estilo m√°s instructivo frente a textos humanos m√°s concretos.\n",
        "\n",
        "**Ranking de t√©cnicas:**\n",
        "\n",
        "1. **Transformers (BERT, Apartado 5)**\n",
        "   \n",
        "   * El *fine-tuning* completo del modelo `bert-base-multilingual-cased` es la t√©cnica m√°s potente. Esto era de esperar ya que los transformers son una alternativa a las RNN cuando se necesitan captuar dependencias largas y relaciones entre tokens complejas.\n",
        "\n",
        "   * El modelo con fine-tuning completo alcanza el mejor accuracy (~0,92) y reduce al m√≠nimo tanto falsos positivos como falsos negativos.\n",
        "\n",
        "   * El an√°lisis de atenciones mostr√≥ que el modelo concentra sus pesos precisamente en los conectores y muletillas detectados en el EDA.\n",
        "\n",
        "2. **Redes secuenciales (LSTM, Apartado 4)**\n",
        "   \n",
        "   * La LSTM con embeddings ajustables (Word2Vec) es la segunda mejor t√©cnica.\n",
        "\n",
        "   * El modelo con embeddings aprendidos desde cero mejora a los modelos cl√°sicos, pero la segunda versi√≥n alcanza un accuracy alrededor del 0,86.\n",
        "\n",
        "   * Esta mejora tiene sentido ya que la LSTM tiene memoria por lo que puede tener en cuenta secuencias a corto plazo y no solo bolsas de palabras. En este corpus en concreto, esto es clave porque el patr√≥n de la IA est√° sobre todo en c√≥mo se encadenan las palabras.\n",
        "\n",
        "3. **Modelos cl√°sicos con TF-IDF (Apartado 2)**\n",
        "   \n",
        "   * El mejor modelo cl√°sico fue una SVM lineal / Regresi√≥n Log√≠stica sobre una representaci√≥n TF-IDF.\n",
        "\n",
        "   * Esta configuraci√≥n se situ√≥ en torno al 0,78 de accuracy.\n",
        "\n",
        "   * Usar N-Gramas permite al modelo tener algo de contexto lo que permite capturar algunas f√≥rmulas instructivas de la IA.\n",
        "\n",
        "   * En cambio, el modelo con Word2Vec promediado perdi√≥ rendimiento, porque al hacer la media de los vectores se destruye justo la parte que importa en este dataset: la estructura secuencial y el tono instructivo.\n",
        "\n",
        "En conjunto, las t√©cnicas que mejor funcionan son aquellas capaces de modelar expl√≠citamente la secuencia y el contexto."
      ],
      "metadata": {
        "id": "1pauS1-gNGdG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7.2. Limitaciones de los modelos cl√°sicos frente a DL y Transformers\n",
        "\n",
        "1. **Modelos cl√°sicos: alta dimensionalidad y rigidez**\n",
        "   Los modelos basados en frecuencias sufren de:\n",
        "\n",
        "   * Alta dimensionalidad y dependencia de un vocabulario fijo.\n",
        "   * Ausencia de sem√°ntica contextual: una palabra tiene el mismo vector en cualquier frase.\n",
        "\n",
        "   * En la pr√°ctica, estos modelos fncionan bien cuando encuentrar coincidencias de N-gramas pero no consiguen relacionar dependecias largas.\n",
        "\n",
        "2. **Word2Vec promediado: sem√°ntica sin estructura**\n",
        "\n",
        "   * Esto aplana el estilo, se pierden las pistas secuenciales y discursivas que diferencian textos IA de humanos.\n",
        "\n",
        "   * El resultado emp√≠rico ha sido un rendimiento inferior al de TF-IDF, lo que confirma que la sem√°ntica sin estructura no basta para esta tarea.\n",
        "\n",
        "3. **LSTM: mejora el contexto, pero con limitaciones**\n",
        "\n",
        "   La LSTM corrige una parte importante del problema: procesa la secuencia en orden y mantiene un estado de memoria. Sin embargo, hay que tener en cuenta algunas desventajas:\n",
        "\n",
        "   * Procesamiento estrictamente secuencial por lo que el entrenamiento es lento.\n",
        "\n",
        "   * Dificultades para capturar dependencias muy largas, aunque mejor que una RNN simple.\n",
        "     \n",
        "    * En este caso, la LSTM mejora los resultados frente a los modelos cl√°sico pero no aprovecha el contexto tan bien como los Transformers.\n",
        "\n",
        "4. **Transformers**\n",
        "\n",
        "   Los Transformers resuelven muchas de las limitaciones anteriores, son la soluci√≥n al cuello de botella secuencial:\n",
        "\n",
        "   * El mecanismo de auto-atenci√≥n permite que cada token se pueda relacionar con cualquier otro, sin importar la distancia.\n",
        "\n",
        "   * El procesamiento es **paralelo** en vez de secuencial, por lo que el coste computacional es menor."
      ],
      "metadata": {
        "id": "v12Uz59UPzl-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7.3. Trade-offs entre interpretabilidad, coste computacional y rendimiento\n",
        "\n",
        "Los resultados de la pr√°ctica ilustran la relaci\n",
        "√≥n entre: **Rendimiento**, **Coste computacional** y **Interpretabilidad**\n",
        "\n",
        "1. **Modelos cl√°sicos (Regresi√≥n Log√≠stica, Naive Bayes, SVM)**\n",
        "\n",
        "   * **Interpretabilidad:** alta  \n",
        "     * Es sencillo inspeccionar pesos y saber qu√© palabras y N-gramas empujan hacia cada clase.\n",
        "\n",
        "   * **Coste computacional:** muy bajo  \n",
        "     * Entrenan en segundos, lo que permite probar muchas configuraciones sin casi coste.\n",
        "\n",
        "   * **Rendimiento:** bueno  \n",
        "     * Hay mucho margen de mejora, sobre todo en falsos negativos (IA indetificado como Humano).\n",
        "\n",
        "2. **LSTM bidireccional**  \n",
        "   * **Interpretabilidad:** media\n",
        "     * Se pueden analizar ejemplos, curvas de entrenamiento y comparar errores por clase aunque es m√°s dificil que en el modelo anterior.\n",
        "   * **Coste computacional:** medio\n",
        "     * Requiere GPU para entrenar y que los tiempo de entrenamiento no se disparen.\n",
        "   * **Rendimiento:** notable\n",
        "     * Supera a los modelos cl√°sicos y corrige parte de sus errores estructurales, sobre todo en textos con patrones secuenciales claros.\n",
        "\n",
        "3. **Transformers con fine-tuning ligero (√∫ltima capa)**  \n",
        "   * **Interpretabilidad:** baja  \n",
        "     * El modelo es complejo, pero se puede razonar a partir de m√©tricas y, adem√°s, el coste computacional no es desorbitado  \n",
        "   * **Coste computacional:** medio-alto pero asumible\n",
        "     * Solo se actualizan las √∫ltimas capas, lo que reduce el tiempo sin perder demasiado rendimiento.  \n",
        "   * **Rendimiento:** muy alto  \n",
        "     * Supera a la LSTM, y se acerca al techo del conjunto de datos.\n",
        "\n",
        "4. **Transformer con fine-tuning completo**  \n",
        "   * **Interpretabilidad:** baja  \n",
        "     * Herramientas como `bertviz` permiten ver qu√© tokens reciben m√°s atenci√≥n, pero no son resultados tan f√°ciles de identificar\n",
        "   * **Coste computacional:** alto\n",
        "     * El tiempo de entrenamiento se multiplica, ya que se actualizan todas las capas (~170M de par√°metros).  \n",
        "   * **Rendimiento:** m√°ximo\n",
        "     * Es el modelo que mejor se ajusta al corpus, reduce m√°s los errores y explota tanto el l√©xico como el estilo.\n",
        "\n",
        "En aplicaciones reales y tras analizar tanto los resultados del rendimiento comoel coste computacional, yo sugiero que:\n",
        "\n",
        "* Si se necesita una soluci√≥n r√°pida y explicable, un modelo cl√°sico con TF-IDF puede ser suficiente.\n",
        "\n",
        "* Si se busca un equilibrio entre coste y rendimiento, la LSTM o el fine-tuning ligero de BERT son las mejores opciones.\n",
        "\n",
        "* El fine-tuning completo de un Transformer solo se justifica cuando cada punto de accuracy adicional tiene un valor alto y se dispone de recursos computacionales suficientes."
      ],
      "metadata": {
        "id": "2fy59VS-P4ul"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7.4. Visualizaciones complementarias\n",
        "\n",
        "A lo largo de la pr√°ctica se han utilizado m√∫ltiples visualizaciones parciales como histogramas de longitud, nubes de palabras,  matrices de confusi√≥n... En este apartado, se sintetizan los resultados de rendimiento y contexto en una tabla.\n",
        "\n",
        "Adem√°s, es importante mencionar que las matrices de confusi√≥n permiten ver claramente c√≥mo: El transformer reduce ambos errores pero sobre todo los falsos negativos (textos generados con IA clasificados como Humanos) que son los m√°s frecuentes en este corpus."
      ],
      "metadata": {
        "id": "v-ToCzVxP9-c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tabla_modelos = pd.DataFrame(\n",
        "    {\n",
        "        \"Familia de modelo\": [\n",
        "            \"Cl√°sico\",\n",
        "            \"Cl√°sico\",\n",
        "            \"Deep Learning secuencial\",\n",
        "            \"Transformer (ligero)\",\n",
        "            \"Transformer (completo)\",\n",
        "        ],\n",
        "        \"T√©cnica espec√≠fica\": [\n",
        "            \"TF-IDF (1‚Äì3 N-grams)\",\n",
        "            \"Word2Vec promediado\",\n",
        "            \"LSTM + Word2Vec ajustable\",\n",
        "            \"BERT + fine-tuning parcial\",\n",
        "            \"BERT con fine-tuning total\",\n",
        "        ],\n",
        "        \"Accuracy (test) aprox.\": [\n",
        "            \"~0,78\",\n",
        "            \"~0,65\",\n",
        "            \"~0,81\",\n",
        "            \"~0,88‚Äì0,90\",\n",
        "            \"~0,92\",\n",
        "        ],\n",
        "        \"Capacidad de contexto\": [\n",
        "            \"Baja (contexto local de N-gramas)\",\n",
        "            \"Nula para este corpus (pierde estilo)\",\n",
        "            \"Media (contexto secuencial, retiene solo dependencias cortas/medias)\",\n",
        "            \"Alta (contexto global con pocas capas entrenables)\",\n",
        "            \"Muy alta (contexto global, modelo totalmente especializado)\",\n",
        "        ],\n",
        "    }\n",
        ")\n",
        "print(f\"\\033[1mResumen comparativo de modelos\\033[0m\")\n",
        "tabla_modelos"
      ],
      "metadata": {
        "id": "49v_JEWfkdul"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7.5. Reflexi√≥n cr√≠tica final sobre los resultados y el aprendizaje obtenido\n",
        "\n",
        "Tras cumplimentar toda la pr√°ctica se puede concluir que:\n",
        "\n",
        "* El **EDA** (Apartado 1) proporcion√≥ una hip√≥tesis clara:\n",
        "  * La diferencia entre humano e IA est√° principalmente en el estilo instructivo y la presencia de conectores, m√°s que en el tema o la longitud.\n",
        "\n",
        "* Los **modelos cl√°sicos** (Apartado 2) funcionaron tal y como se esperaba, BoW y TF-IDF: buenos cuando el l√©xico basta, pero limitados cuando el orden y el contexto global son importantes.\n",
        "\n",
        "* El **modelado de t√≥picos** (Apartado 3) valid√≥ la hip√≥tesis desde un enfoque no supervisado, al encontrar t√≥picos claramente asociados a estilo en todo el Dataset.\n",
        "\n",
        "* Las **LSTM** (Apartado 4) demostraron que introducir memoria y secuencia mejora los resultados, pero tambi√©n que el coste crece aunque las dependencias largas est√°n fuera de su alcance.\n",
        "\n",
        "* Los **Transformers** (Apartado 5) confirmaron su ventaja para esta tarea en concreto.\n",
        "\n",
        "* Finalmente, el **an√°lisis de atenciones** (Apartado 6) confirmo las hip√≥tesis: el modelo no solo funciona bien, sino que lo hace por las **mismas razones** que se identificaron en el EDA. Atiende a conectores, muletillas y estructuras instructivas, y falla cuando el estilo no es claro.\n",
        "\n",
        "Al mismo tiempo, el an√°lisis de errores (falsos positivos y falsos negativos) sac√≥ a la luz los l√≠mites del modelo: cuando una IA escribe con estilo m√°s humano o un humano adopta un tono m√°s estructurado, el clasificador se confunde. En pocas palabras, el modelo ha aprendido un criterio aproximado del tipo estilo instructivo o formal = IA y estilo m√°s concreto y coloquial = humano, m√°s que una distinci√≥n genuina entre ‚ÄúHumano vs IA‚Äù. Esto supone una debilidad cr√≠tica: el sistema tiende a asociar escritura  ‚Äúlimpia‚Äù y automatizada con IA, por lo que es fr√°gil y podr√≠a ser enga√±ado con relativa facilidad si un humano adopta sistem√°ticamente un estilo muy normativo o si una IA imita un registro m√°s desordenado, coloquial o narrativo."
      ],
      "metadata": {
        "id": "YqkKuH-XQBff"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "En resumen, los resultados reflejan un panorama coherente: los modelos cl√°sicos con TF-IDF ofrecen una base s√≥lida y f√°cilmente interpretable, las LSTM mejoran el rendimiento al incorporar contexto secuencial y el Transformer con fine-tuning completo marca el techo de precisi√≥n a costa de un mayor coste computacional. El EDA inicial, el modelado de t√≥picos y el an√°lisis de atenciones concluyen en la misma idea: la diferencia entre textos humanos e IA est√° sobre todo en el estilo discursivo. A partir de ah√≠, la elecci√≥n del modelo adecuado depender√° menos de cu√°l es el que mejor rendimiento tiene y m√°s del equilibrio que se quiera asumir entre recursos, explicabilidad y nivel de exigencia en el rendimiento.\n"
      ],
      "metadata": {
        "id": "BCVErlAk277t"
      }
    }
  ]
}